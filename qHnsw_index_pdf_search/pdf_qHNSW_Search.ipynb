{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3280b01a-d3b7-4ef6-9494-789d15bc48ec",
      "metadata": {
        "id": "3280b01a-d3b7-4ef6-9494-789d15bc48ec"
      },
      "source": [
        "# Semantic Search on PDF Documents with qHNSW Index\n",
        "\n",
        "##### Note: This example requires a KDB.AI endpoint and API key. Sign up for a free [KDB.AI account](https://kdb.ai/get-started).\n",
        "\n",
        "This example demonstrates how to use KDB.AI to run semantic search on unstructured text documents.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Tip:</b> This sample uses ‘qHNSW’ , a new vector index choice in KDB.AI. It will support the same API options as the existing ‘HNSW’ index but with the significant difference that the index is stored on-disk and memory-mapped as required. This means data inserts will have negligible memory and cpu footprints. The vector index can grow and be searched as long as there is disk space available. Among other cases, this stands out as a great index for memory contrained situations such as edge devices.\n",
        "</div>\n",
        "\n",
        "Semantic search allows users to perform searches based on the meaning or similarity of the data rather than exact matches. It works by converting the query into a vector representation and then finding similar vectors in the database. This way, even if the query and the data in the database are not identical, the system can identify and retrieve the most relevant results based on their semantic meaning.\n",
        "\n",
        "### Aim\n",
        "In this tutorial, we'll walk you through the process of performing semantic search on documents, taking PDFs as example, using KDB.AI as the vector store. We will cover the following topics:\n",
        "\n",
        "0. Setup\n",
        "1. Load PDF Data\n",
        "2. KDB.AI Table Creation (qHNSW index)\n",
        "3. LlamaIndex index & query_engine setup\n",
        "4. Retrieve Similar Sentences & RAG\n",
        "5. Delete the KDB.AI Table\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75362bc9",
      "metadata": {
        "id": "75362bc9"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda3d787",
      "metadata": {
        "id": "dda3d787"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "In order to successfully run this sample, note the following steps depending on where you are running this notebook:\n",
        "\n",
        "-***Run Locally / Private Environment:*** The [Setup](https://github.com/KxSystems/kdbai-samples/blob/main/README.md#setup) steps in the repository's `README.md` will guide you on prerequisites and how to run this with Jupyter.\n",
        "\n",
        "\n",
        "-***Colab / Hosted Environment:*** Open this notebook in Colab and run through the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19c5107-001e-40c2-bfe7-6c9c99e4846d",
      "metadata": {
        "id": "b19c5107-001e-40c2-bfe7-6c9c99e4846d"
      },
      "source": [
        "### Set Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8bad9d73",
      "metadata": {
        "id": "8bad9d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://pypi.org/simple, https://kx-user-read:****@ext-nexus.kxi-dev.kx.com/repository/kxi/simple\n",
            "Requirement already satisfied: llama-index in /home/gflood/.local/lib/python3.10/site-packages (0.10.59)\n",
            "Requirement already satisfied: llama-index-llms-openai in /home/gflood/.local/lib/python3.10/site-packages (0.1.27)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /home/gflood/.local/lib/python3.10/site-packages (0.1.11)\n",
            "Requirement already satisfied: llama-index-readers-file in /home/gflood/.local/lib/python3.10/site-packages (0.1.32)\n",
            "Requirement already satisfied: llama-index-vector-stores-kdbai in /home/gflood/.local/lib/python3.10/site-packages (0.2.0)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.2.9)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.1.13)\n",
            "Requirement already satisfied: llama-index-core==0.10.59 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.10.59)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.1.8)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.1.7)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/gflood/.local/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.59->llama-index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (3.10.0)\n",
            "Requirement already satisfied: dataclasses-json in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (1.37.1)\n",
            "Requirement already satisfied: pandas in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-core==0.10.59->llama-index) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-readers-file) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: kdbai-client>=1.1.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-vector-stores-kdbai) (1.4.0.dev23)\n",
            "Requirement already satisfied: pykx<3.0.0,>=2.1.1 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-vector-stores-kdbai) (2.5.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/gflood/.local/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
            "Requirement already satisfied: packaging in /home/gflood/.local/lib/python3.10/site-packages (from kdbai-client>=1.1.0->llama-index-vector-stores-kdbai) (23.2)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.0.11)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /home/gflood/.local/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index) (0.4.9)\n",
            "Requirement already satisfied: pytz>=2022.1 in /home/gflood/.local/lib/python3.10/site-packages (from pykx<3.0.0,>=2.1.1->llama-index-vector-stores-kdbai) (2024.1)\n",
            "Requirement already satisfied: toml~=0.10.2 in /home/gflood/.local/lib/python3.10/site-packages (from pykx<3.0.0,>=2.1.1->llama-index-vector-stores-kdbai) (0.10.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.59->llama-index) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /home/gflood/.local/lib/python3.10/site-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: anyio in /home/gflood/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.59->llama-index) (4.4.0)\n",
            "Requirement already satisfied: certifi in /home/gflood/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.59->llama-index) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/gflood/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.59->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/gflood/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.59->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /home/gflood/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.59->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/gflood/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.59->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /home/gflood/.local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.59->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/gflood/.local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.59->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/gflood/.local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.59->llama-index) (2024.7.24)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core==0.10.59->llama-index) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gflood/.local/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.59->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.59->llama-index) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gflood/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.59->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gflood/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.59->llama-index) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/gflood/.local/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.59->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.59->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gflood/.local/lib/python3.10/site-packages (from dataclasses-json->llama-index-core==0.10.59->llama-index) (3.21.3)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gflood/.local/lib/python3.10/site-packages (from anyio->httpx->llama-index-core==0.10.59->llama-index) (1.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/gflood/.local/lib/python3.10/site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /home/gflood/.local/lib/python3.10/site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/gflood/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.59->llama-index) (1.16.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://pypi.org/simple, https://kx-user-read:****@ext-nexus.kxi-dev.kx.com/repository/kxi/simple\n",
            "Requirement already satisfied: pandas in /home/gflood/.local/lib/python3.10/site-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /home/gflood/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gflood/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/gflood/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-vector-stores-kdbai\n",
        "!pip install pandas\n",
        "!pip install kdbai_client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21979389",
      "metadata": {
        "id": "21979389"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "rjfp-08NPdvC",
      "metadata": {
        "id": "rjfp-08NPdvC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.vector_stores.kdbai import KDBAIVectorStore\n",
        "\n",
        "import kdbai_client as kdbai\n",
        "\n",
        "OUTDIR = \"pdf\"\n",
        "RESET = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f23f6513",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23f6513",
        "outputId": "d2832d7e-e91f-4ba4-e996-6057d2d03632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-23 16:11:30--  https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/document_search/data/research_paper.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1206037 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘./data/research_paper.pdf’\n",
            "\n",
            "research_paper.pdf  100%[===================>]   1.15M  4.82MB/s    in 0.2s    \n",
            "\n",
            "2024-09-23 16:11:30 (4.82 MB/s) - ‘./data/research_paper.pdf’ saved [1206037/1206037]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n",
        "### This downloads research paper pdf into your environment\n",
        "if os.path.exists(\"./data/research_paper\") == False:\n",
        "  !mkdir ./data\n",
        "  !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/document_search/data/research_paper.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "yCCdm-QDPtDZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCCdm-QDPtDZ",
        "outputId": "02495a02-0f49-4181-9f38-7a0f8273ecbe"
      },
      "outputs": [],
      "source": [
        "# OpenAI API Key: https://platform.openai.com/api\n",
        "os.environ[\"OPENAI_API_KEY\"] = (\n",
        "    os.environ[\"OPENAI_API_KEY\"]\n",
        "    if \"OPENAI_API_KEY\" in os.environ\n",
        "    else getpass(\"OpenAI API Key: \")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "HCI37LxrPywl",
      "metadata": {
        "id": "HCI37LxrPywl"
      },
      "outputs": [],
      "source": [
        "# Set up LlamaIndex Parameters\n",
        "\n",
        "EMBEDDING_MODEL  = \"text-embedding-3-small\"\n",
        "GENERATION_MODEL = 'gpt-4o-mini'\n",
        "\n",
        "llm = OpenAI(model=GENERATION_MODEL)\n",
        "embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8594c911",
      "metadata": {
        "id": "8594c911"
      },
      "source": [
        "### Configure Console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b870117b",
      "metadata": {
        "id": "b870117b"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_colwidth\", 300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a425b33",
      "metadata": {
        "id": "8a425b33"
      },
      "source": [
        "### Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9a135635",
      "metadata": {
        "id": "9a135635"
      },
      "outputs": [],
      "source": [
        "def show_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    print(df.shape)\n",
        "    return df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48826990",
      "metadata": {
        "id": "48826990"
      },
      "source": [
        "## 1. Load PDF Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2992812-4705-489d-974f-b7b44132343a",
      "metadata": {
        "id": "b2992812-4705-489d-974f-b7b44132343a"
      },
      "source": [
        "### Read Text From PDF Document\n",
        "\n",
        "We LlamaIndex SimpleDirectorReader to read in our PDF file.\n",
        "\n",
        "The PDF we are using is [this research paper](https://arxiv.org/pdf/2308.05801.pdf) presenting information on the formation of Interstellar Objects in the Milky Way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "NWa3S2iwQd_K",
      "metadata": {
        "id": "NWa3S2iwQd_K"
      },
      "outputs": [],
      "source": [
        "reader = SimpleDirectoryReader(\n",
        "    input_dir=\"data\",\n",
        ")\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70505eae-4138-4ba8-80e9-fc13c37d0b32",
      "metadata": {
        "id": "70505eae-4138-4ba8-80e9-fc13c37d0b32"
      },
      "source": [
        "### Define KDB.AI Session\n",
        "\n",
        "KDB.AI comes in two offerings:\n",
        "\n",
        "1. [KDB.AI Cloud](https://trykdb.kx.com/kdbai/signup/) - For experimenting with smaller generative AI projects with a vector database in our cloud.\n",
        "2. [KDB.AI Server](https://trykdb.kx.com/kdbaiserver/signup/) - For evaluating large scale generative AI applications on-premises or on your own cloud provider.\n",
        "\n",
        "Depending on which you use there will be different setup steps and connection details required.\n",
        "\n",
        "##### Option 1. KDB.AI Cloud\n",
        "\n",
        "To use KDB.AI Cloud, you will need two session details - a URL endpoint and an API key.\n",
        "To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
        "\n",
        "You can connect to a KDB.AI Cloud session using `kdbai.Session` and passing the session URL endpoint and API key details from your KDB.AI Cloud portal.\n",
        "\n",
        "If the environment variables `KDBAI_ENDPOINTS` and `KDBAI_API_KEY` exist on your system containing your KDB.AI Cloud portal details, these variables will automatically be used to connect.\n",
        "If these do not exist, it will prompt you to enter your KDB.AI Cloud portal session URL endpoint and API key details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "401f8162",
      "metadata": {
        "id": "401f8162"
      },
      "outputs": [],
      "source": [
        "KDBAI_ENDPOINT = (\n",
        "    os.environ[\"KDBAI_ENDPOINT\"]\n",
        "    if \"KDBAI_ENDPOINT\" in os.environ\n",
        "    else input(\"KDB.AI endpoint: \")\n",
        ")\n",
        "KDBAI_API_KEY = (\n",
        "    os.environ[\"KDBAI_API_KEY\"]\n",
        "    if \"KDBAI_API_KEY\" in os.environ\n",
        "    else getpass(\"KDB.AI API key: \")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3b3a4932",
      "metadata": {
        "id": "3b3a4932"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:You are running a development version of `kdbai_client`.\n",
            "Compatibility with the KDB.AI server is not guaranteed.\n"
          ]
        }
      ],
      "source": [
        "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53a7384",
      "metadata": {
        "id": "b53a7384"
      },
      "source": [
        "##### Option 2. KDB.AI Server\n",
        "\n",
        "To use KDB.AI Server, you will need download and run your own container.\n",
        "To do this, you will first need to sign up for free [here](https://trykdb.kx.com/kdbaiserver/signup/).\n",
        "\n",
        "You will receive an email with the required license file and bearer token needed to download your instance.\n",
        "Follow instructions in the signup email to get your session up and running.\n",
        "\n",
        "Once the [setup steps](https://code.kx.com/kdbai/gettingStarted/kdb-ai-server-setup.html) are complete you can then connect to your KDB.AI Server session using `kdbai.Session` and passing your local endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f48fc536",
      "metadata": {
        "id": "f48fc536"
      },
      "outputs": [],
      "source": [
        "# session = kdbai.Session(endpoint=\"http://localhost:8082\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c6525c",
      "metadata": {
        "id": "48c6525c"
      },
      "source": [
        "### Define Vector DB Table Schema\n",
        "\n",
        "The next step is to define a schema for our KDB.AI table where we will store our embeddings. Our table will have two columns.\n",
        "\n",
        "At this point you will select the index and metric you want to use for searching.\n",
        "\n",
        "In this case, we will use the qHNSW index, Euclidean Distance (L2) for the search metric, and we specify the number of dimensions of our embeddings (384)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "eArvp20fSDc6",
      "metadata": {
        "id": "eArvp20fSDc6"
      },
      "outputs": [],
      "source": [
        "#Set up the schema and indexes for KDB.AI table, specifying embeddings column with 384 dimensions, Euclidean Distance, and flat index\n",
        "pdf_schema = [\n",
        "    {\"name\": \"document_id\", \"type\": \"bytes\"},\n",
        "    {\"name\": \"text\", \"type\": \"bytes\"},\n",
        "    {\"name\": \"embedding\", \"type\": \"float64s\"}\n",
        "]\n",
        "\n",
        "indexes = [\n",
        "    {\n",
        "        \"name\": \"qhnsw_index\",\n",
        "        \"type\": \"qHnsw\",\n",
        "        \"column\": \"embedding\",\n",
        "        \"params\": {\"dims\": 1536, \"metric\": \"L2\"},\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518cfe1e",
      "metadata": {
        "id": "518cfe1e"
      },
      "source": [
        "### Create Vector DB Table\n",
        "\n",
        "Use the KDB.AI `create_table` function to create a table that matches the defined schema in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6e670f9e",
      "metadata": {
        "id": "6e670f9e"
      },
      "outputs": [],
      "source": [
        "# get the database connection. Default database name is 'default'\n",
        "database = session.database('default')\n",
        "\n",
        "# First ensure the table does not already exist\n",
        "try:\n",
        "    database.table(\"pdf\").drop()\n",
        "except kdbai.KDBAIException:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e1d190db-7c19-418e-9140-3dff04c9d4c0",
      "metadata": {
        "id": "e1d190db-7c19-418e-9140-3dff04c9d4c0"
      },
      "outputs": [],
      "source": [
        "table = database.create_table(\"pdf\", schema = pdf_schema, indexes = indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466068bc",
      "metadata": {
        "id": "466068bc"
      },
      "source": [
        "We can use `query` to see our table exists but is empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "74e7332e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "74e7332e",
        "outputId": "e07a811b-44a7-428a-9252-3a10ac15e035"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document_id</th>\n",
              "      <th>text</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [document_id, text, embedding]\n",
              "Index: []"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table.query()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YvcNh1v_Wixe",
      "metadata": {
        "id": "YvcNh1v_Wixe"
      },
      "source": [
        "## 3. LlamaIndex index & query_engine setup\n",
        "Define the index: using KDB.AI as the vector store, chunk, embed, and load the document into KDB.AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "He42izi1Soee",
      "metadata": {
        "id": "He42izi1Soee"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'list' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m KDBAIVectorStore(table)\n\u001b[1;32m      3\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m----> 4\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSentenceSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/core/indices/base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     transformations,\n\u001b[1;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:78\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     72\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:314\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    307\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    308\u001b[0m ):\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:285\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:239\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[1;32m    238\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_with_embedding(nodes_batch, show_progress)\n\u001b[0;32m--> 239\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node, new_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nodes_batch, new_ids):\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;66;03m# NOTE: remove embedding from node to avoid duplication\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/vector_stores/kdbai/base.py:129\u001b[0m, in \u001b[0;36mKDBAIVectorStore.add\u001b[0;34m(self, nodes, **add_kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table, kdbai\u001b[38;5;241m.\u001b[39mTable):\n\u001b[0;32m--> 129\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table, kdbai\u001b[38;5;241m.\u001b[39mTablePyKx):\n\u001b[1;32m    131\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mschema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ],
      "source": [
        "vector_store = KDBAIVectorStore(table)\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    transformations=[SentenceSplitter(chunk_size=2048, chunk_overlap=0)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31a4ecd",
      "metadata": {
        "id": "e31a4ecd"
      },
      "source": [
        "### Verify Data Has Been Inserted\n",
        "\n",
        "Running `table.query()` should show us that data has been added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6ecb8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ee6ecb8d",
        "outputId": "07564fc5-6d97-4e58-cfbd-0dbb6a9f8ba7"
      },
      "outputs": [],
      "source": [
        "show_df(table.query())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tiZ5HWsEThcT",
      "metadata": {
        "id": "tiZ5HWsEThcT"
      },
      "source": [
        "#### Set up the LlamaIndex Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "u_IlkIcuTn3e",
      "metadata": {
        "id": "u_IlkIcuTn3e"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf8650d",
      "metadata": {
        "id": "8bf8650d"
      },
      "source": [
        "## 4. Retrieve Similar Sentences & RAG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ddb725-e0e7-4c00-a22b-3234eacf6bd1",
      "metadata": {
        "id": "31ddb725-e0e7-4c00-a22b-3234eacf6bd1"
      },
      "source": [
        "Now that the embeddings are stored in KDB.AI, we can perform semantic search using through the LlamaIndex query engine.\n",
        "\n",
        "### Search 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b24c1179",
      "metadata": {
        "id": "b24c1179"
      },
      "outputs": [],
      "source": [
        "#search_term1 = \"The Milky Way is thought to host a huge population of interstellar objects (ISOs), numbering\"\n",
        "search_term1 = \"what is the estimated population of interstellar objects (ISOs) in the milky way\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QbyVfwB8VTcg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbyVfwB8VTcg",
        "outputId": "c0dc4596-9b69-409b-9abd-b130e3b010e2"
      },
      "outputs": [],
      "source": [
        "retrieved_chunks = query_engine.retrieve(search_term1)\n",
        "for i in retrieved_chunks:\n",
        "    print(i.node.get_text())\n",
        "    print(\"____________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9R4vgFZMVvv9",
      "metadata": {
        "id": "9R4vgFZMVvv9"
      },
      "source": [
        "We can see these sentences do reference our search term 'number of interstellar objects in the milky way' in some way."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n0n7OglvVvvA",
      "metadata": {
        "id": "n0n7OglvVvvA"
      },
      "source": [
        "### Now we can perform RAG, passing the retrieved chunks from above to the LLM for a generate response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ttZpktxET3tq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttZpktxET3tq",
        "outputId": "8395b5b0-6645-46ba-f56a-97069c10afb9"
      },
      "outputs": [],
      "source": [
        "result = query_engine.query(search_term1)\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91762de7",
      "metadata": {
        "id": "91762de7"
      },
      "source": [
        "### Search 2\n",
        "\n",
        "Let's try another search term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e7486a9c",
      "metadata": {
        "id": "e7486a9c"
      },
      "outputs": [],
      "source": [
        "search_term2 = \"how does planet formation occur\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6765075c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6765075c",
        "outputId": "b70454e9-0436-4266-fba6-65852ad68939"
      },
      "outputs": [],
      "source": [
        "retrieved_chunks = query_engine.retrieve(search_term2)\n",
        "for i in retrieved_chunks:\n",
        "    print(i.node.get_text())\n",
        "    print(\"____________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ce337f",
      "metadata": {
        "id": "25ce337f"
      },
      "source": [
        "Again, we can see these sentences do reference our search term 'how does planet formation occur' in some way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3ce969",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3ce969",
        "outputId": "c7e3988b-7e6e-4abd-e6d9-8b0d0805df1c"
      },
      "outputs": [],
      "source": [
        "result = query_engine.query(search_term2)\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e93878",
      "metadata": {
        "id": "f6e93878"
      },
      "source": [
        "## 5. Delete the KDB.AI Table\n",
        "\n",
        "Once finished with the table, it is best practice to drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74b6f20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d74b6f20",
        "outputId": "2f4b3592-0290-48b4-c280-7bd3f8e4d46a"
      },
      "outputs": [],
      "source": [
        "table.drop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346d56db",
      "metadata": {
        "id": "346d56db"
      },
      "source": [
        "## Take Our Survey\n",
        "\n",
        "We hope you found this sample helpful! Your feedback is important to us, and we would appreciate it if you could take a moment to fill out our brief survey. Your input helps us improve our content.\n",
        "\n",
        "[**Take the Survey**](https://delighted.com/t/vm88uwcB)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
