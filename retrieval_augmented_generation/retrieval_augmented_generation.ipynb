{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with LangChain\n",
    "\n",
    "##### Note: This example requires a KDB.AI endpoint and API key. Sign up for a free [KDB.AI account](https://kdb.ai/get-started).\n",
    "\n",
    "This example will demonstrate how to use an advanced prompt engineering technique called Retrieval Augmented Generation (RAG), with hands-on examples using Langchain, KDB.AI and various LLMs.\n",
    "\n",
    "### What is RAG and Why Do We Need it?\n",
    "\n",
    "Large Language Models have remarkable capabilities in generating human-like text. These models are found in applications ranging from chatbots to content generation and translation. However, they face a significant challenge in staying up-to-date with recent world events, as they are essentially frozen in time, operating within the static knowledge snapshot captured during their training.\n",
    "\n",
    "To bridge this gap and address the need for specialized, real-time information, the concept of \"Retrieval Augmented Generation\" (RAG) has emerged as a powerful solution. RAG enables these language models to access relevant data from external knowledge bases, enriching their responses with current and contextually accurate information. For more content on RAG you can check out our videos on [Youtube](https://www.youtube.com/@KxSystems/streams) where we discuss the best practices for RAG, chunking strategies, the variety of approaches as well as how to evaluate your RAG application.\n",
    "\n",
    "### Aim\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "\n",
    "1. Load Text Data\n",
    "1. Define OpenAI Text Emedding Model\n",
    "1. Store Embeddings In KDB.AI\n",
    "1. Search For Similar Documents To A Given Query\n",
    "1. Perform Retrieval Augmented Generation\n",
    "1. Delete the KDB.AI Table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba816",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d42eb",
   "metadata": {},
   "source": [
    "### Install dependencies \n",
    "\n",
    "In order to successfully run this sample, note the following steps depending on where you are running this notebook:\n",
    "\n",
    "-***Run Locally / Private Environment:*** The [Setup](https://github.com/KxSystems/kdbai-samples/blob/main/README.md#setup) steps in the repository's `README.md` will guide you on prerequisites and how to run this with Jupyter.\n",
    "\n",
    "\n",
    "-***Colab / Hosted Environment:*** Open this notebook in Colab and run through the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb407992",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kdbai_client langchain langchain_openai langchain-huggingface #langchain-community \n",
    "\n",
    "import os\n",
    "!git clone -b KDBAI_v1.4 https://github.com/KxSystems/langchain.git\n",
    "os.chdir('langchain/libs/community')\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n",
    "### This downloads State of the Union speech data\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"./data/state_of_the_union.txt\") == False:\n",
    "    !mkdir ./data\n",
    "    !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36ecd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49f72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273bab",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "To follow this example you will need to request both an [OpenAI API Key](https://platform.openai.com/apps) and a [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens). \n",
    "\n",
    "You can create both for free by registering using the links provided. Once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4470f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "159357d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = (\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "    if \"HUGGINGFACEHUB_API_TOKEN\" in os.environ\n",
    "    else getpass(\"Hugging Face API Token: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708acb6e",
   "metadata": {},
   "source": [
    "## 1. Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f921b0",
   "metadata": {},
   "source": [
    "### Read In Text Document\n",
    "\n",
    "The document we will use for this examples is a State of the Union message from the President of the United States to the United States Congress.\n",
    "\n",
    "In the below code snippet, we read the text file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b70f894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents we want to prompt an LLM about\n",
    "doc = TextLoader(\"data/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d9450",
   "metadata": {},
   "source": [
    "### Split The Document Into Chunks\n",
    "\n",
    "We then split this document into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "451745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50dc02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "pages = [p.page_content for p in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b685aa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d8c79",
   "metadata": {},
   "source": [
    "## 2. Define OpenAI Text Embedding Model\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database. We select `text-embedding-3-small` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f680c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659a7a2",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings In KDB.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d6e2",
   "metadata": {},
   "source": [
    "With the embeddings created, we need to store them in a vector database to enable efficient searching.\n",
    "\n",
    "### Define KDB.AI Session\n",
    "\n",
    "KDB.AI comes in two offerings:\n",
    "\n",
    "1. [KDB.AI Cloud](https://trykdb.kx.com/kdbai/signup/) - For experimenting with smaller generative AI projects with a vector database in our cloud.\n",
    "2. [KDB.AI Server](https://trykdb.kx.com/kdbaiserver/signup/) - For evaluating large scale generative AI applications on-premises or on your own cloud provider.\n",
    "\n",
    "Depending on which you use there will be different setup steps and connection details required.\n",
    "\n",
    "##### Option 1. KDB.AI Cloud\n",
    "\n",
    "To use KDB.AI Cloud, you will need two session details - a URL endpoint and an API key.\n",
    "To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
    "\n",
    "You can connect to a KDB.AI Cloud session using `kdbai.Session` and passing the session URL endpoint and API key details from your KDB.AI Cloud portal.\n",
    "\n",
    "If the environment variables `KDBAI_ENDPOINTS` and `KDBAI_API_KEY` exist on your system containing your KDB.AI Cloud portal details, these variables will automatically be used to connect.\n",
    "If these do not exist, it will prompt you to enter your KDB.AI Cloud portal session URL endpoint and API key details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8e7b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    "    if \"KDBAI_ENDPOINT\" in os.environ\n",
    "    else input(\"KDB.AI endpoint: \")\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    "    if \"KDBAI_API_KEY\" in os.environ\n",
    "    else getpass(\"KDB.AI API key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cfdc8",
   "metadata": {},
   "source": [
    "##### Option 2. KDB.AI Server\n",
    "\n",
    "To use KDB.AI Server, you will need download and run your own container.\n",
    "To do this, you will first need to sign up for free [here](https://trykdb.kx.com/kdbaiserver/signup/). \n",
    "\n",
    "You will receive an email with the required license file and bearer token needed to download your instance.\n",
    "Follow instructions in the signup email to get your session up and running.\n",
    "\n",
    "Once the [setup steps](https://code.kx.com/kdbai/gettingStarted/kdb-ai-server-setup.html) are complete you can then connect to your KDB.AI Server session using `kdbai.Session` and passing your local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a77bf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = kdbai.Session(endpoint=\"http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec574f8",
   "metadata": {},
   "source": [
    "### Define Vector DB Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6977019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_schema = [\n",
    "    {\"name\": \"id\", \"type\": \"str\"},\n",
    "    {\"name\": \"text\", \"type\": \"bytes\"},\n",
    "    {\"name\": \"embeddings\", \"type\": \"float32s\"},\n",
    "]\n",
    "\n",
    "indexes = [{'name': 'flat_index', 'column': 'embeddings', 'type': 'flat', 'params': {\"dims\": 1536, \"metric\": \"L2\"}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5c4c1",
   "metadata": {},
   "source": [
    "### Create Vector DB Table\n",
    "\n",
    "Use the KDB.AI `create_table` function to create a table that matches the defined schema in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a844f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = session.database(\"default\")\n",
    "\n",
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    database.table(\"rag_langchain\").drop()\n",
    "except kdbai.KDBAIException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "596eaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = database.create_table(\"rag_langchain\", schema=rag_schema, indexes=indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b465157",
   "metadata": {},
   "source": [
    "### Add Embedded Data to KDB.AI Table\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.add_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47a92e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1f71fa4d-b876-4ede-9c2f-a2feab358e0f',\n",
       " '617e8958-bd38-4a0c-b0da-c85e939a6d5a',\n",
       " '45745afe-596d-4b4a-b6f5-ca380cbb17bb',\n",
       " '005773e8-4410-4ac1-b703-d69a944673aa',\n",
       " 'c8befd91-211b-4f03-88e3-ebca313a19a0',\n",
       " '51044021-f668-410c-848d-d587d124843f',\n",
       " '4ec0daf2-4a3c-4e6a-ac7b-7a7611dd80a6',\n",
       " 'ee9fe9bc-10e6-4ed5-a10f-3b5400b1bd6f',\n",
       " 'e62fd355-d776-4ba0-a94f-6fb757576e4a',\n",
       " '77bd54db-51e8-4c42-80fa-4961431b2c3a',\n",
       " '4955a466-2e4d-45b3-b349-b8cba5e0b958',\n",
       " 'e8467e1e-c977-4096-8366-1e2104bdf8f2',\n",
       " 'eac91746-237d-420e-9933-b2a7bc94e0ff',\n",
       " '05db44c8-4aef-4514-be16-17c08fa81691',\n",
       " '1b89fd3d-523e-4596-8386-8d75de457162',\n",
       " '5eedfebf-e420-4543-a3d5-b9b72987c372',\n",
       " '9821af3d-0e02-45ee-8431-4489b2680043',\n",
       " '76aa0e0c-bfc3-4a9c-9208-4304e649451d',\n",
       " '62578d27-54bf-4aaa-9cca-07e19a216710',\n",
       " '76bdc65e-f5d4-4ab3-9a41-5a30bd39b62e',\n",
       " '2f80e016-3962-45b8-85ed-4eadb1016eb6',\n",
       " 'f25ff936-dba3-4c4b-9bdf-f8db4bb1889b',\n",
       " '27630a25-ef27-445a-a6da-4545e84a8f78',\n",
       " '10558419-924f-41b7-ac37-cc85fe55c04e',\n",
       " 'ce28729c-1f8b-41e9-9f43-04b739aeaf62',\n",
       " '8feb4be5-266d-4bb7-9100-73444979a177',\n",
       " '66c4e541-e4a9-4b40-9f37-2109e0fb5e24',\n",
       " 'a3d9ffb2-dd42-4864-85d1-0a7fe769140a',\n",
       " '8a2d8985-bdf4-48ec-b7f1-f7baecb13054',\n",
       " '134039e3-d6a2-4b6b-a62a-a084bdbf12e6',\n",
       " '60e36112-2f31-4903-9cc6-e6673a208cc5',\n",
       " '33676eee-4c54-40bf-93f3-4a1557ffb24a',\n",
       " 'e59bac37-c9ee-4332-91dc-0dd9719f72a0',\n",
       " 'eb8efc86-cd27-45d6-8e91-d34190a98056',\n",
       " '233d2801-5777-4a17-802b-0e05ac11f0cf',\n",
       " '126e9eb5-cee9-4e99-85d9-2ea7c9ac1de8',\n",
       " '1990039c-99bb-4321-bf19-855fd432bc77',\n",
       " '908f620a-a502-4d20-94a2-6e82b080e7f6',\n",
       " '19f037a7-a9fe-424e-b0e1-533b416477ee',\n",
       " '000e9207-429a-4165-a8f5-12c052cfe18c',\n",
       " '5c2d269d-767d-455a-9f97-5fc0b1843ae7',\n",
       " 'd824fb20-f768-4fb1-a10b-44015821c1d6',\n",
       " 'd99edf66-1d34-4b70-a06d-e6cbc39c56ac',\n",
       " '4f508142-b782-47b7-8ecb-bb04f8bcec4b',\n",
       " '94c2c9f9-a041-4c95-8295-ba39c6fa8918',\n",
       " 'ba0e56c9-5c50-4d9b-b6c0-9c002086b80c',\n",
       " '6108ae84-9802-4cfa-a971-4c079cbbae10',\n",
       " '6522c800-cfdb-42e8-90e8-546ca59cc724',\n",
       " 'c68b1ea0-e60b-448f-b0e8-8cc27af060b2',\n",
       " '122302d1-b952-4ca6-9bb7-e269cc809279',\n",
       " 'a613031a-382e-4235-a691-d73aec452075',\n",
       " '6fa5e799-fe77-4ffc-ba1e-18eaf722cc03',\n",
       " '1c2e68d1-172a-4529-a7db-c7cd8375a496',\n",
       " 'bac974d0-26d7-4b3c-9bf4-149b04acfb42',\n",
       " 'ca062164-2d26-485a-9348-1f58a1e0c0e6',\n",
       " '3d76aa08-263c-4c02-9e25-160093c21242',\n",
       " '40bc3c68-7021-4d7d-bf58-9cc98e554e1e',\n",
       " '28cfadf6-3384-489a-a7d5-193d5c07150b',\n",
       " '26b9428a-7bd5-4d1c-89f4-2e444d9b2362',\n",
       " '716db85e-2320-4ea0-b6a2-7d8f2abd1002',\n",
       " '10d0431b-ed29-4f7a-944e-6d9f36a742bb',\n",
       " '09ff2442-a92d-4e04-b024-faffc5d35f71',\n",
       " '3c7ece65-650a-4bdc-9087-4748220199ab',\n",
       " '394b0d6a-6257-40d5-993f-9afb975cdcbf',\n",
       " '1a6ee425-a92f-4404-a595-1eca62ff8653',\n",
       " '6f216b48-75bd-41ec-8ef3-9cd5c1127735',\n",
       " '787e9fc2-30ec-4db7-8383-d4f1442aee13',\n",
       " '2d4d06c5-75f0-48cd-bfb2-eaaf1f868562',\n",
       " 'd45a8bf5-3da6-4370-8358-4c9f14bd5dc4',\n",
       " '12b7bae0-f1ea-4cd8-82df-c4083de7eea1',\n",
       " 'cbcb1288-955d-41f1-b59f-efb0cb2aa1e8',\n",
       " '4401444b-3948-49cf-ba5d-a64b92193961',\n",
       " 'fc95858d-a3e4-4b10-8d64-4297d59b0502',\n",
       " '280e473a-7b53-4e37-be5a-ed2856c3ede1',\n",
       " '60e0ce95-9b94-4ddd-ba5d-67f30212c4f4',\n",
       " '9e00388c-4390-4d2e-9ddf-66f318f5a309',\n",
       " '130a71d0-fbd0-4a01-832d-edbde7bfb4a2',\n",
       " 'adc1dacc-f057-4979-a01c-7522b71bb459',\n",
       " 'fc9985f6-457e-48ab-8b53-5a4e80f3947f',\n",
       " '17fb831f-1600-488e-b3d3-b89010fb1f92',\n",
       " '5e4b36c4-1413-4e9f-9bf0-1f830314a740',\n",
       " 'd78af270-22e6-4bd0-af06-64b54cc69745',\n",
       " '8ea6e0b3-71f1-4d02-88c3-33d87de5a5c6',\n",
       " 'c66eeae2-86b7-4d38-81be-b81fb151ef8d',\n",
       " '02eb3423-4eaf-40be-931e-8629a7b4f4b1',\n",
       " 'cc71202e-e2ce-4952-a81d-3cc27942b712',\n",
       " 'f1a65b30-fa03-4c30-81e5-58c20f421612',\n",
       " '23b9b1af-ca12-41c4-b62c-eac095fe2b6e',\n",
       " 'ff1cc9e4-7ee7-41f1-a674-cb0f1171e267']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "vecdb_kdbai.add_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a28271",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb116f8",
   "metadata": {},
   "source": [
    "## 4. Search For Similar Documents To A Given Query \n",
    "\n",
    "Before we implement RAG, let's see an example of using similarity search directly on KDB.AI vector store. The search uses Euclidean similarity search which measures distance between two points in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a17c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the nations strengths?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63aee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "query_sim = vecdb_kdbai.similarity_search(query, index='flat_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8c9b456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': '23b9b1af-ca12-41c4-b62c-eac095fe2b6e', 'embeddings': array([ 0.03127478,  0.0130375 ,  0.04139533, ..., -0.01439452,\n",
       "        -0.01379844, -0.0240585 ], dtype=float32)})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecfec",
   "metadata": {},
   "source": [
    "This result returns the most similar chunks of text to our query, which is an okay start but it is hard to read. It would be a lot better if we could summarize these findings and return a response that is more human readable - this is where RAG comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a865",
   "metadata": {},
   "source": [
    "## 5. Perform Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa29b8",
   "metadata": {},
   "source": [
    "There are four different ways to do [question answering (QA) in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4):\n",
    "- `load_qa_chain` will do QA over all documents passed every time it is called. It is simple and comprehensive, but can be slower and less efficient than `RetrievalQA` as it may not focus on the most relevant parts of the tests for the question. In one example below, we will perform similarity search with KDB.AI before using `load_qa_chain` to act upon \"all documents\" being passed.\n",
    "- `RetrievalQA` retrieves the most relevant chunk of text and does QA on that subset. It uses `load_qa_chain` under the hood on each chunk and is faster and more efficient then the vanilla `load_qa_chain`. These performance gains come at the risk of losing some information or context from the documents as it may not always find the best text chunks for the question. In one example below, we will use KDB.AI as the retriever of `RetrievalQA`.\n",
    "- `VectorstoreIndexCreator` is a higher level wrapper for `RetrievalQA` to make it easier to run in fewer lines of code\n",
    "- `ConversationalRetrievalChain` builds on RetrievalQAChain to provide a chat history component\n",
    "\n",
    "In this tutorial we will implement the first two.\n",
    "\n",
    "### 'load_qa_chain' with OpenAI and HuggingFace LLMs\n",
    "\n",
    "We set up two question-answering chains for different models, OpenAI and HuggingFaceHub, using LangChain's `load_qa_chain` function. To do this we first perform the same similarity search run earlier and then run both chains on the query and the related chunks from the documentation, printing the responses from both models. We compare the responses of OpenAI and HuggingFaceHub models to the query about vector database strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f29a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/gflood/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# select two llm models (OpenAI gpt-4o, HuggingFaceHub mistralai/Mistral-7B-Instruct-v0.2)\n",
    "llm_openai = ChatOpenAI(model=\"gpt-4o\", max_tokens=512)\n",
    "llm_mistral = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22959c2",
   "metadata": {},
   "source": [
    "\n",
    "We chose the `chain_type =\"stuff\"` which is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32a49086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_mistral, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b63d6afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': '23b9b1af-ca12-41c4-b62c-eac095fe2b6e', 'embeddings': array([ 0.03127478,  0.0130375 ,  0.04139533, ..., -0.01439452,\n",
       "        -0.01379844, -0.0240585 ], dtype=float32)})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the most related chunks to the query\n",
    "query_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e281391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The nations' strengths as highlighted in the context include the ability to turn every crisis into an opportunity, being defined by possibilities, and the strength and resilience of the American people. The overall message is that the nation is strong and continues to grow stronger over time.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI - run the chain on the query and the related chunks from the documentation\n",
    "chain_openAI.invoke({'input_documents':query_sim, 'question':query})['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f590804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The nation is strong because the American people are strong and full of possibilities. The country has turned every crisis into an opportunity and continues to do so. It is defined by its ability to see and create possibilities. The nation is stronger today than it was a year ago and will be stronger a year from now.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HugginFace - run the chain on the query and the related chunks from the documentation\n",
    "chain_HuggingFaceHub.invoke({'input_documents':query_sim, 'question':query})['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b79d56",
   "metadata": {},
   "source": [
    "### RetrievalQA with GPT-4o\n",
    "\n",
    "Let's try the second method using `RetrievalQA`. This time lets use GPT-4o as our LLM of choice.\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-4o for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1a6b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ddaa8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "qabot = RetrievalQA.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0),\n",
    "    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K, index=\"flat_index\")),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b59a1",
   "metadata": {},
   "source": [
    "`as_retriever` is a method that converts a vectorstore into a retriever. A retriever is an interface that returns documents given an unstructured query. By using <code>as_retriever</code>, we can create a retriever from a vectorstore and use it to retrieve relevant documents for a query. This allows us to perform question answering over the documents indexed by the vectorstore `vecdb_kdbai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d1ba3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the nations strengths?\n",
      "-----\n",
      "The strengths of the United States, as highlighted in the context, include:\n",
      "\n",
      "1. **Resilience and Adaptability**: The nation has a history of turning crises into opportunities.\n",
      "2. **Possibilities**: The country is defined by the concept of possibilities, indicating a forward-looking and optimistic outlook.\n",
      "3. **Strength of the People**: The American people are described as strong, contributing to the overall strength of the nation.\n",
      "4. **Diplomacy and Resolve**: American diplomacy and resolve are emphasized as important factors in international relations.\n",
      "5. **Military Preparedness**: The U.S. has mobilized ground forces, air squadrons, and ship deployments to protect NATO allies.\n",
      "6. **Economic Sanctions and Coalition Building**: The U.S. has built a coalition of nations to impose economic sanctions on Russia and support Ukraine.\n",
      "7. **Historical Achievements**: The nation has a legacy of fighting for freedom, expanding liberty, and defeating totalitarianism and terror.\n",
      "8. **Infrastructure Investment**: The passage of the Bipartisan Infrastructure Law is seen as a significant investment in rebuilding and improving the nation's infrastructure.\n",
      "9. **Unity and Collective Action**: The emphasis on unity and collective action as one people, one America, is a key strength.\n",
      "\n",
      "These strengths contribute to the nation's ability to meet and overcome current and future challenges.\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "print(\"-----\")\n",
    "print(qabot.invoke(dict(query=query))[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a3d7a",
   "metadata": {},
   "source": [
    "Trying another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ed67c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qabot(qabot, query: str):\n",
    "    print(new_query)\n",
    "    print(\"---\")\n",
    "    return qabot.invoke(dict(query=new_query))[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1b517b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the things this country needs to protect?\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This country needs to protect several key areas:\\n\\n1. **American Jobs and Businesses**: By ensuring taxpayer dollars support American jobs and businesses through initiatives like Buy American policies.\\n2. **Safety and Security**: By investing in crime prevention, community policing, and measures to reduce gun violence, such as universal background checks and banning assault weapons and high-capacity magazines.\\n3. **Immigration and Border Security**: By providing pathways to citizenship for certain groups, revising laws to meet labor needs, and securing borders with new technology and joint patrols.\\n4. **Voting Rights**: By protecting the fundamental right to vote and ensuring that votes are counted, combating laws that suppress or subvert elections.\\n5. **Liberty and Justice**: By advancing immigration reform, protecting women's rights, and holding law enforcement accountable.\\n6. **National and International Security**: By maintaining strong American diplomacy and resolve, particularly in response to international conflicts like Russia's attack on Ukraine.\\n\\nThese areas are crucial for maintaining the country's integrity, safety, and prosperity.\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = \"what are the things this country needs to protect?\"\n",
    "query_qabot(qabot, new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7574cc",
   "metadata": {},
   "source": [
    "Clearly, Retrieval Augmented Generation stands out as a valuable technique that synergizes the capabilities of language models such as GPT-3 with the potency of information retrieval.\n",
    "By enhancing the input with contextually specific data, RAG empowers language models to produce responses that are not only more precize but also well-suited to the context. \n",
    "Particularly in enterprize scenarios where extensive fine-tuning may not be feasible, RAG presents an efficient and economically viable approach to deliver personalized and informed interactions with users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0568a",
   "metadata": {},
   "source": [
    "## 6. Delete the KDB.AI Table\n",
    "\n",
    "Once finished with the table, it is best practice to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf0e3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a102d",
   "metadata": {},
   "source": [
    "## Take Our Survey\n",
    "\n",
    "We hope you found this sample helpful! Your feedback is important to us, and we would appreciate it if you could take a moment to fill out our brief survey. Your input helps us improve our content.\n",
    "\n",
    "[**Take the Survey**](https://delighted.com/t/dgCLUkdx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
