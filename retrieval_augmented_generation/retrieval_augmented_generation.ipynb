{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with LangChain\n",
    "\n",
    "This example will demonstrate how to use an advanced prompt engineering technique called Retrieval Augmented Generation (RAG), with hands-on examples using Langchain, KDB.AI and various LLMs.\n",
    "\n",
    "### What is RAG and Why Do We Need it?\n",
    "\n",
    "Large Language Models have remarkable capabilities in generating human-like text. These models are found in applications ranging from chatbots to content generation and translation. However, they face a significant challenge in staying up-to-date with recent world events, as they are essentially frozen in time, operating within the static knowledge snapshot captured during their training.\n",
    "\n",
    "To bridge this gap and address the need for specialized, real-time information, the concept of \"Retrieval Augmented Generation\" (RAG) has emerged as a powerful solution. RAG enables these language models to access relevant data from external knowledge bases, enriching their responses with current and contextually accurate information.\n",
    "\n",
    "### Aim\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "\n",
    "1. Load Text Data\n",
    "1. Define OpenAI Text Emedding Model\n",
    "1. Store Embeddings In KDB.AI\n",
    "1. Search For Similar Documents To A Given Query\n",
    "1. Perform Retrieval Augmented Generation\n",
    "1. Delete the KDB.AI Table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba816",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb407992",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ecd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import KDBAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273bab",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "To follow this example you will need to request both an [OpenAI API Key](https://platform.openai.com/apps) and a [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens). \n",
    "\n",
    "You can create both for free by registering using the links provided. Once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4470f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159357d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face API Token:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = (\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "    if \"HUGGINGFACEHUB_API_TOKEN\" in os.environ\n",
    "    else getpass(\"Hugging Face API Token: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708acb6e",
   "metadata": {},
   "source": [
    "## 1. Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f921b0",
   "metadata": {},
   "source": [
    "### Read In Text Document\n",
    "\n",
    "In the below code snippet, we read the text file and split the document into chunks. This document is a State of the Union message from the President of the United States to the United States Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70f894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents we want to prompt an LLM about\n",
    "doc = TextLoader(\"data/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d9450",
   "metadata": {},
   "source": [
    "### Split The Document Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50dc02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "pages = [p.page_content for p in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b685aa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d8c79",
   "metadata": {},
   "source": [
    "## 2. Define OpenAI Text Embedding Model\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database. We select `text-embedding-ada-002` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f680c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659a7a2",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings In KDB.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d6e2",
   "metadata": {},
   "source": [
    "With the embeddings created, we need to store them in a vector database to enable efficient searching. KDB.AI is perfect for this task.\n",
    "\n",
    "KDB.AI comes in two offerings:\n",
    "\n",
    "1. [KDB.AI Cloud](https://trykdb.kx.com/kdbai/signup/) - For experimenting with smaller generative AI projects with a vector database in our cloud.\n",
    "2. [KDB.AI Server](https://trykdb.kx.com/kdbaiserver/signup/) - For evaluating large scale generative AI applications on-premises or on your own cloud provider.\n",
    "\n",
    "Depending on which you use there will be different setup steps and connection details required.\n",
    "\n",
    "### KDB.AI Cloud\n",
    "\n",
    "To use KDB.AI Cloud, you will need two session details - a URL endpoint and an API key. To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
    "\n",
    "You can connect to a KDB.AI Cloud session using `kdbai.Session` and passing the session URL endpoint and API key details from your KDB.AI Cloud portal.\n",
    "\n",
    "If the environment variables `KDBAI_ENDPOINTS` and `KDBAI_API_KEY` exist on your system containing your KDB.AI Cloud portal details, these variables will automatically be used to connect.\n",
    "If these do not exist, it will prompt you to enter your KDB.AI Cloud portal session URL endpoint and API key details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e7b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    "    if \"KDBAI_ENDPOINT\" in os.environ\n",
    "    else input(\"KDB.AI endpoint: \")\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    "    if \"KDBAI_API_KEY\" in os.environ\n",
    "    else getpass(\"KDB.AI API key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23e2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cfdc8",
   "metadata": {},
   "source": [
    "### KDB.AI Server\n",
    "\n",
    "To use KDB.AI Server, you will need download and run your own container. To do this you will first need to sign up for free [here](https://trykdb.kx.com/kdbaiserver/signup/). \n",
    "\n",
    "You will receive an email with the required license file and bearer token needed to download your instance. Follow instructions in the signup email to get your session up and running.\n",
    "\n",
    "Once the [setup steps](https://code.kx.com/kdbai/gettingStarted/kdb-ai-server-setup.html) are complete you can then connect to your KDB.AI Server session using `kdbai.Session` and passing your local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = kdbai.Session(endpoint='http://localhost:8082')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec574f8",
   "metadata": {},
   "source": [
    "### Define Vector DB Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6977019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"id\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"text\", \"pytype\": \"bytes\"},\n",
    "        {\n",
    "            \"name\": \"embeddings\",\n",
    "            \"pytype\": \"float32\",\n",
    "            \"vectorIndex\": {\"dims\": 1536, \"metric\": \"L2\", \"type\": \"flat\"},\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5c4c1",
   "metadata": {},
   "source": [
    "### Create Vector DB Table\n",
    "\n",
    "Use the KDB.AI `create_table` function to create a table that matches the defined schema in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a844f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"rag_langchain\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596eaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = session.create_table(\"rag_langchain\", rag_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b465157",
   "metadata": {},
   "source": [
    "### Add Embedded Data to KDB.AI Table\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.from_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47a92e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "vecdb_kdbai.add_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a28271",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb116f8",
   "metadata": {},
   "source": [
    "## 4. Search For Similar Documents To A Given Query \n",
    "\n",
    "Before we implement RAG, let's see an example of using similarity search directly on KDB.AI vector store. The search uses Euclidean similarity search which measures distance between two points in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the nations strengths?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63aee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "query_sim = vecdb_kdbai.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c9b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': 'e19475e6-e0df-4bbb-95e6-388956795a38', 'embeddings': array([-0.00213418, -0.01874734,  0.0001179 , ..., -0.00102042,\n",
      "         0.00753241, -0.02074311], dtype=float32)})]"
     ]
    }
   ],
   "source": [
    "query_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecfec",
   "metadata": {},
   "source": [
    "This result returns the most similar chunks of text to our query, which is an okay start but it is hard to read. It would be a lot better if we could summarize these findings and return a response that is more human readable - this is where RAG comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a865",
   "metadata": {},
   "source": [
    "## 5. Perform Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa29b8",
   "metadata": {},
   "source": [
    "There are four different ways to do [question answering (QA) in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4):\n",
    "- `load_qa_chain` will do QA over all documents passed every time it is called. It is simple and comprehensive, but can be slower and less efficient than `RetrievalQA` as it may not focus on the most relevant parts of the tests for the question. In one example below, we will perform similarity search with KDB.AI before using `load_qa_chain` to act upon \"all documents\" being passed.\n",
    "- `RetrievalQA` retrieves the most relevant chunk of text and does QA on that subset. It uses `load_qa_chain` under the hood on each chunk and is faster and more efficient then the vanilla `load_qa_chain`. These performance gains come at the risk of losing some information or context from the documents as it may not always find the best text chunks for the question. In one example below, we will use KDB.AI as the retriever of `RetrievalQA`.\n",
    "- `VectorstoreIndexCreator` is a higher level wrapper for `RetrievalQA` to make it easier to run in fewer lines of code\n",
    "- `ConversationalRetrievalChain` builds on RetrievalQAChain to provide a chat history component\n",
    "\n",
    "In this tutorial we will implement the first two.\n",
    "\n",
    "### 'load_qa_chain' with OpenAI and HuggingFace LLMs\n",
    "\n",
    "We set up two question-answering chains for different models, OpenAI and HuggingFaceHub, using LangChain's `load_qa_chain` function. To do this we first perform the same similarity search run earlier and then run both chains on the query and the related chunks from the documentation, printing the responses from both models. We compare the responses of OpenAI and HuggingFaceHub models to the query about vector database strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f29a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two llm models (OpenAI text-davinci-003, HuggingFaceHub google/flan-t5-xxl(designed for short answers))\n",
    "llm_openai = OpenAI(model=\"text-davinci-003\", max_tokens=512)\n",
    "llm_flan = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22959c2",
   "metadata": {},
   "source": [
    "\n",
    "We chose the `chain_type =\"stuff\"` which is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a49086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_flan, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b63d6afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': 'e19475e6-e0df-4bbb-95e6-388956795a38', 'embeddings': array([-0.00213418, -0.01874734,  0.0001179 , ..., -0.00102042,\n",
      "         0.00753241, -0.02074311], dtype=float32)})]"
     ]
    }
   ],
   "source": [
    "# Show the most related chunks to the query\n",
    "query_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e281391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' The American people are strong. The nation is capable of turning crises into opportunities and is filled with possibilities.'"
     ]
    }
   ],
   "source": [
    "# OpenAI - run the chain on the query and the related chunks from the documentation\n",
    "chain_openAI.run(input_documents=query_sim, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f590804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We are strong today than we were a year ago. And we will be stronger a year'"
     ]
    }
   ],
   "source": [
    "# HugginFace - run the chain on the query and the related chunks from the documentation\n",
    "chain_HuggingFaceHub.run(input_documents=query_sim, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c74a9",
   "metadata": {},
   "source": [
    "We can see the response from OpenAI is longer and more detailed and seems to have done a better job summarizing the nation's strengths from the document provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b79d56",
   "metadata": {},
   "source": [
    "### RetrievalQA with GPT-3.5\n",
    "\n",
    "Let's try the second method using `RetrievalQA`. This time lets use GPT-3.5 as our LLM of choice.\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-3.5 Turbo for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1a6b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddaa8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "qabot = RetrievalQA.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.0),\n",
    "    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b59a1",
   "metadata": {},
   "source": [
    "`as_retriever` is a method that converts a vectorstore into a retriever. A retriever is an interface that returns documents given an unstructured query. By using <code>as_retriever</code>, we can create a retriever from a vectorstore and use it to retrieve relevant documents for a query. This allows us to perform question answering over the documents indexed by the vectorstore `vecdb_kdbai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1ba3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the nations strengths?\n",
      "-----\n",
      "The passage does not explicitly mention the specific strengths of the United States. However, it emphasizes the resilience and strength of the American people, their ability to turn crises into opportunities, and their commitment to protecting freedom, expanding liberty, and defending democracy. It also highlights the nation's history of debating important issues, fighting for freedom, and building a strong and prosperous nation. Additionally, it mentions the mobilization of American forces to defend NATO allies and the implementation of economic sanctions to support Ukraine.\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "print(\"-----\")\n",
    "print(qabot(dict(query=query))[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a3d7a",
   "metadata": {},
   "source": [
    "Trying another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ed67c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qabot(qabot, query: str):\n",
    "    print(new_query)\n",
    "    print(\"---\")\n",
    "    return qabot(dict(query=new_query))[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1b517b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the things this country needs to protect?\n",
      "-----\n",
      "\"Based on the provided context, the country needs to protect the following:\\n\\n1. The right to vote and ensure that every vote is counted.\\n2. The torch of liberty that has led generations of immigrants to the United States.\\n3. The pathway to citizenship for Dreamers, temporary status holders, farm workers, and essential workers.\\n4. The border security by implementing new technology, joint patrols, and dedicated immigration judges.\\n5. American industries and jobs by buying American-made products and leveling the playing field with competitors like China.\\n6. Communities by investing in crime prevention, community police officers, and holding law enforcement accountable.\\n7. Access to healthcare, including preserving a woman's right to choose and advancing maternal health care.\\n8. LGBTQ+ rights by passing the bipartisan Equality Act.\\n9. Democracy by protecting it from threats and ensuring fairness and opportunity for all.\\n10. American jobs and businesses by using taxpayer dollars to support American products and industries.\""
     ]
    }
   ],
   "source": [
    "new_query = \"what are the things this country needs to protect?\"\n",
    "query_qabot(qabot, new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7574cc",
   "metadata": {},
   "source": [
    "Clearly, Retrieval Augmented Generation stands out as a valuable technique that synergizes the capabilities of language models such as GPT-3 with the potency of information retrieval.\n",
    "By enhancing the input with contextually specific data, RAG empowers language models to produce responses that are not only more precize but also well-suited to the context. \n",
    "Particularly in enterprize scenarios where extensive fine-tuning may not be feasible, RAG presents an efficient and economically viable approach to deliver personalized and informed interactions with users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0568a",
   "metadata": {},
   "source": [
    "## 6. Delete the KDB.AI Table\n",
    "\n",
    "Once finished with the table, it is best practice to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf0e3026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
