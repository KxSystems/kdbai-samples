{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with LangChain\n",
    "\n",
    "##### Note: This example requires a KDB.AI endpoint and API key. Sign up for a free [KDB.AI account](https://kdb.ai/get-started).\n",
    "\n",
    "This example will demonstrate how to use an advanced prompt engineering technique called Retrieval Augmented Generation (RAG), with hands-on examples using Langchain, KDB.AI and various LLMs.\n",
    "\n",
    "### What is RAG and Why Do We Need it?\n",
    "\n",
    "Large Language Models have remarkable capabilities in generating human-like text. These models are found in applications ranging from chatbots to content generation and translation. However, they face a significant challenge in staying up-to-date with recent world events, as they are essentially frozen in time, operating within the static knowledge snapshot captured during their training.\n",
    "\n",
    "To bridge this gap and address the need for specialized, real-time information, the concept of \"Retrieval Augmented Generation\" (RAG) has emerged as a powerful solution. RAG enables these language models to access relevant data from external knowledge bases, enriching their responses with current and contextually accurate information. For more content on RAG you can check out our videos on [Youtube](https://www.youtube.com/@KxSystems/streams) where we discuss the best practices for RAG, chunking strategies, the variety of approaches as well as how to evaluate your RAG application.\n",
    "\n",
    "### Aim\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "\n",
    "1. Load Text Data\n",
    "1. Define OpenAI Text Emedding Model\n",
    "1. Store Embeddings In KDB.AI\n",
    "1. Search For Similar Documents To A Given Query\n",
    "1. Perform Retrieval Augmented Generation\n",
    "1. Delete the KDB.AI Table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba816",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d42eb",
   "metadata": {},
   "source": [
    "### Install dependencies \n",
    "\n",
    "In order to successfully run this sample, note the following steps depending on where you are running this notebook:\n",
    "\n",
    "-***Run Locally / Private Environment:*** The [Setup](https://github.com/KxSystems/kdbai-samples/blob/main/README.md#setup) steps in the repository's `README.md` will guide you on prerequisites and how to run this with Jupyter.\n",
    "\n",
    "\n",
    "-***Colab / Hosted Environment:*** Open this notebook in Colab and run through the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb407992",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fac8d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://kx-user-read:****@ext-nexus.kxi-dev.kx.com/repository/kxi/simple\n",
      "Requirement already satisfied: kdbai_client in /home/gflood/.local/lib/python3.10/site-packages (1.4.0.dev10)\n",
      "Requirement already satisfied: langchain in /home/gflood/.local/lib/python3.10/site-packages (0.1.15)\n",
      "Requirement already satisfied: langchain_openai in /home/gflood/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: langchain-community in /home/gflood/.local/lib/python3.10/site-packages (0.0.32)\n",
      "Requirement already satisfied: langchain-huggingface in /home/gflood/.local/lib/python3.10/site-packages (0.0.3)\n",
      "Requirement already satisfied: packaging in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (23.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (2.1.4)\n",
      "Requirement already satisfied: pykx<3.0.0,>=2.1.1 in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (2.5.1)\n",
      "Requirement already satisfied: requests in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (3.10.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain_openai) (1.37.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /home/gflood/.local/lib/python3.10/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain-huggingface) (0.25.1)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain-huggingface) (2.7.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain-huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain-huggingface) (4.44.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gflood/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/gflood/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: filelock in /home/gflood/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/gflood/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gflood/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/gflood/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/gflood/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gflood/.local/lib/python3.10/site-packages (from pandas>=1.5.0->kdbai_client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas>=1.5.0->kdbai_client) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas>=1.5.0->kdbai_client) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gflood/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/gflood/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: toml~=0.10.2 in /home/gflood/.local/lib/python3.10/site-packages (from pykx<3.0.0,>=2.1.1->kdbai_client) (0.10.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (2024.7.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/gflood/.local/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /home/gflood/.local/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /home/gflood/.local/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /home/gflood/.local/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/gflood/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/gflood/.local/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/gflood/.local/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gflood/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gflood/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/gflood/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gflood/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->kdbai_client) (1.16.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/gflood/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/gflood/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/gflood/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/gflood/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.37.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/gflood/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/gflood/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kdbai_client langchain langchain_openai langchain-community langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e9b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-25 14:30:05--  https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39027 (38K) [text/plain]\n",
      "Saving to: ‘./data/state_of_the_union.txt.5’\n",
      "\n",
      "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2024-09-25 14:30:05 (2.08 MB/s) - ‘./data/state_of_the_union.txt.5’ saved [39027/39027]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n",
    "### This downloads State of the Union speech data\n",
    "!mkdir ./data \n",
    "!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ecd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49f72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273bab",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "To follow this example you will need to request both an [OpenAI API Key](https://platform.openai.com/apps) and a [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens). \n",
    "\n",
    "You can create both for free by registering using the links provided. Once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4470f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159357d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = (\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "    if \"HUGGINGFACEHUB_API_TOKEN\" in os.environ\n",
    "    else getpass(\"Hugging Face API Token: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708acb6e",
   "metadata": {},
   "source": [
    "## 1. Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f921b0",
   "metadata": {},
   "source": [
    "### Read In Text Document\n",
    "\n",
    "The document we will use for this examples is a State of the Union message from the President of the United States to the United States Congress.\n",
    "\n",
    "In the below code snippet, we read the text file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b70f894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents we want to prompt an LLM about\n",
    "doc = TextLoader(\"data/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d9450",
   "metadata": {},
   "source": [
    "### Split The Document Into Chunks\n",
    "\n",
    "We then split this document into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "451745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50dc02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "pages = [p.page_content for p in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b685aa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d8c79",
   "metadata": {},
   "source": [
    "## 2. Define OpenAI Text Embedding Model\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database. We select `text-embedding-3-small` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f680c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659a7a2",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings In KDB.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d6e2",
   "metadata": {},
   "source": [
    "With the embeddings created, we need to store them in a vector database to enable efficient searching.\n",
    "\n",
    "### Define KDB.AI Session\n",
    "\n",
    "KDB.AI comes in two offerings:\n",
    "\n",
    "1. [KDB.AI Cloud](https://trykdb.kx.com/kdbai/signup/) - For experimenting with smaller generative AI projects with a vector database in our cloud.\n",
    "2. [KDB.AI Server](https://trykdb.kx.com/kdbaiserver/signup/) - For evaluating large scale generative AI applications on-premises or on your own cloud provider.\n",
    "\n",
    "Depending on which you use there will be different setup steps and connection details required.\n",
    "\n",
    "##### Option 1. KDB.AI Cloud\n",
    "\n",
    "To use KDB.AI Cloud, you will need two session details - a URL endpoint and an API key.\n",
    "To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
    "\n",
    "You can connect to a KDB.AI Cloud session using `kdbai.Session` and passing the session URL endpoint and API key details from your KDB.AI Cloud portal.\n",
    "\n",
    "If the environment variables `KDBAI_ENDPOINTS` and `KDBAI_API_KEY` exist on your system containing your KDB.AI Cloud portal details, these variables will automatically be used to connect.\n",
    "If these do not exist, it will prompt you to enter your KDB.AI Cloud portal session URL endpoint and API key details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8e7b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    "    if \"KDBAI_ENDPOINT\" in os.environ\n",
    "    else input(\"KDB.AI endpoint: \")\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    "    if \"KDBAI_API_KEY\" in os.environ\n",
    "    else getpass(\"KDB.AI API key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b23e2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are running a development version of `kdbai_client`.\n",
      "Compatibility with the KDB.AI server is not guaranteed.\n"
     ]
    }
   ],
   "source": [
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cfdc8",
   "metadata": {},
   "source": [
    "##### Option 2. KDB.AI Server\n",
    "\n",
    "To use KDB.AI Server, you will need download and run your own container.\n",
    "To do this, you will first need to sign up for free [here](https://trykdb.kx.com/kdbaiserver/signup/). \n",
    "\n",
    "You will receive an email with the required license file and bearer token needed to download your instance.\n",
    "Follow instructions in the signup email to get your session up and running.\n",
    "\n",
    "Once the [setup steps](https://code.kx.com/kdbai/gettingStarted/kdb-ai-server-setup.html) are complete you can then connect to your KDB.AI Server session using `kdbai.Session` and passing your local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a77bf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = kdbai.Session(endpoint=\"http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec574f8",
   "metadata": {},
   "source": [
    "### Define Vector DB Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6977019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_schema = [\n",
    "    {\"name\": \"id\", \"type\": \"str\"},\n",
    "    {\"name\": \"text\", \"type\": \"bytes\"},\n",
    "    {\"name\": \"embeddings\", \"type\": \"float32s\"},\n",
    "]\n",
    "\n",
    "indexes = [{'name': 'flat_index', 'column': 'embeddings', 'type': 'flat', 'params': {\"dims\": 1536, \"metric\": \"L2\"}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5c4c1",
   "metadata": {},
   "source": [
    "### Create Vector DB Table\n",
    "\n",
    "Use the KDB.AI `create_table` function to create a table that matches the defined schema in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a844f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = session.database(\"default\")\n",
    "\n",
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    database.table(\"rag_langchain\").drop()\n",
    "except kdbai.KDBAIException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "596eaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = database.create_table(\"rag_langchain\", schema=rag_schema, indexes=indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b465157",
   "metadata": {},
   "source": [
    "### Add Embedded Data to KDB.AI Table\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.add_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47a92e3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Table.insert() got an unexpected keyword argument 'warn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use KDBAI as vector store\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vecdb_kdbai \u001b[38;5;241m=\u001b[39m KDBAI(table, embeddings)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvecdb_kdbai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/kdbai.py:150\u001b[0m, in \u001b[0;36mKDBAI.add_texts\u001b[0;34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         batch_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     out_ids \u001b[38;5;241m=\u001b[39m out_ids \u001b[38;5;241m+\u001b[39m batch_ids\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_ids\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/kdbai.py:97\u001b[0m, in \u001b[0;36mKDBAI._insert\u001b[0;34m(self, texts, ids, metadata)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, metadata], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Table.insert() got an unexpected keyword argument 'warn'"
     ]
    }
   ],
   "source": [
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "vecdb_kdbai.add_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a28271",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb116f8",
   "metadata": {},
   "source": [
    "## 4. Search For Similar Documents To A Given Query \n",
    "\n",
    "Before we implement RAG, let's see an example of using similarity search directly on KDB.AI vector store. The search uses Euclidean similarity search which measures distance between two points in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the nations strengths?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63aee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "query_sim = vecdb_kdbai.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c9b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': 'e19475e6-e0df-4bbb-95e6-388956795a38', 'embeddings': array([-0.00213418, -0.01874734,  0.0001179 , ..., -0.00102042,\n",
      "         0.00753241, -0.02074311], dtype=float32)})]"
     ]
    }
   ],
   "source": [
    "query_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecfec",
   "metadata": {},
   "source": [
    "This result returns the most similar chunks of text to our query, which is an okay start but it is hard to read. It would be a lot better if we could summarize these findings and return a response that is more human readable - this is where RAG comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a865",
   "metadata": {},
   "source": [
    "## 5. Perform Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa29b8",
   "metadata": {},
   "source": [
    "There are four different ways to do [question answering (QA) in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4):\n",
    "- `load_qa_chain` will do QA over all documents passed every time it is called. It is simple and comprehensive, but can be slower and less efficient than `RetrievalQA` as it may not focus on the most relevant parts of the tests for the question. In one example below, we will perform similarity search with KDB.AI before using `load_qa_chain` to act upon \"all documents\" being passed.\n",
    "- `RetrievalQA` retrieves the most relevant chunk of text and does QA on that subset. It uses `load_qa_chain` under the hood on each chunk and is faster and more efficient then the vanilla `load_qa_chain`. These performance gains come at the risk of losing some information or context from the documents as it may not always find the best text chunks for the question. In one example below, we will use KDB.AI as the retriever of `RetrievalQA`.\n",
    "- `VectorstoreIndexCreator` is a higher level wrapper for `RetrievalQA` to make it easier to run in fewer lines of code\n",
    "- `ConversationalRetrievalChain` builds on RetrievalQAChain to provide a chat history component\n",
    "\n",
    "In this tutorial we will implement the first two.\n",
    "\n",
    "### 'load_qa_chain' with OpenAI and HuggingFace LLMs\n",
    "\n",
    "We set up two question-answering chains for different models, OpenAI and HuggingFaceHub, using LangChain's `load_qa_chain` function. To do this we first perform the same similarity search run earlier and then run both chains on the query and the related chunks from the documentation, printing the responses from both models. We compare the responses of OpenAI and HuggingFaceHub models to the query about vector database strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f29a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two llm models (OpenAI gpt-4o, HuggingFaceHub mistralai/Mistral-7B-Instruct-v0.2)\n",
    "llm_openai = ChatOpenAI(model=\"gpt-4o\", max_tokens=512)\n",
    "llm_mistral = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22959c2",
   "metadata": {},
   "source": [
    "\n",
    "We chose the `chain_type =\"stuff\"` which is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a49086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_mistral, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b63d6afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': 'e19475e6-e0df-4bbb-95e6-388956795a38', 'embeddings': array([-0.00213418, -0.01874734,  0.0001179 , ..., -0.00102042,\n",
      "         0.00753241, -0.02074311], dtype=float32)})]"
     ]
    }
   ],
   "source": [
    "# Show the most related chunks to the query\n",
    "query_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e281391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' The American people are strong. The nation is capable of turning crises into opportunities and is filled with possibilities.'"
     ]
    }
   ],
   "source": [
    "# OpenAI - run the chain on the query and the related chunks from the documentation\n",
    "chain_openAI.invoke({'input_documents':query_sim, 'question':query})['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f590804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We are strong today than we were a year ago. And we will be stronger a year'"
     ]
    }
   ],
   "source": [
    "# HugginFace - run the chain on the query and the related chunks from the documentation\n",
    "chain_HuggingFaceHub.invoke({'input_documents':query_sim, 'question':query})['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b79d56",
   "metadata": {},
   "source": [
    "### RetrievalQA with GPT-4o\n",
    "\n",
    "Let's try the second method using `RetrievalQA`. This time lets use GPT-4o as our LLM of choice.\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-4o for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1a6b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddaa8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "qabot = RetrievalQA.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0),\n",
    "    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b59a1",
   "metadata": {},
   "source": [
    "`as_retriever` is a method that converts a vectorstore into a retriever. A retriever is an interface that returns documents given an unstructured query. By using <code>as_retriever</code>, we can create a retriever from a vectorstore and use it to retrieve relevant documents for a query. This allows us to perform question answering over the documents indexed by the vectorstore `vecdb_kdbai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1ba3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the nations strengths?\n",
      "-----\n",
      "The passage does not explicitly mention the specific strengths of the United States. However, it emphasizes the resilience and strength of the American people, their ability to turn crises into opportunities, and their commitment to protecting freedom, expanding liberty, and defending democracy. It also highlights the nation's history of debating important issues, fighting for freedom, and building a strong and prosperous nation. Additionally, it mentions the mobilization of American forces to defend NATO allies and the implementation of economic sanctions to support Ukraine.\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "print(\"-----\")\n",
    "print(qabot.invoke(dict(query=query))[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a3d7a",
   "metadata": {},
   "source": [
    "Trying another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ed67c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qabot(qabot, query: str):\n",
    "    print(new_query)\n",
    "    print(\"---\")\n",
    "    return qabot.invoke(dict(query=new_query))[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1b517b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the things this country needs to protect?\n",
      "-----\n",
      "\"Based on the provided context, the country needs to protect the following:\\n\\n1. The right to vote and ensure that every vote is counted.\\n2. The torch of liberty that has led generations of immigrants to the United States.\\n3. The pathway to citizenship for Dreamers, temporary status holders, farm workers, and essential workers.\\n4. The border security by implementing new technology, joint patrols, and dedicated immigration judges.\\n5. American industries and jobs by buying American-made products and leveling the playing field with competitors like China.\\n6. Communities by investing in crime prevention, community police officers, and holding law enforcement accountable.\\n7. Access to healthcare, including preserving a woman's right to choose and advancing maternal health care.\\n8. LGBTQ+ rights by passing the bipartisan Equality Act.\\n9. Democracy by protecting it from threats and ensuring fairness and opportunity for all.\\n10. American jobs and businesses by using taxpayer dollars to support American products and industries.\""
     ]
    }
   ],
   "source": [
    "new_query = \"what are the things this country needs to protect?\"\n",
    "query_qabot(qabot, new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7574cc",
   "metadata": {},
   "source": [
    "Clearly, Retrieval Augmented Generation stands out as a valuable technique that synergizes the capabilities of language models such as GPT-3 with the potency of information retrieval.\n",
    "By enhancing the input with contextually specific data, RAG empowers language models to produce responses that are not only more precize but also well-suited to the context. \n",
    "Particularly in enterprize scenarios where extensive fine-tuning may not be feasible, RAG presents an efficient and economically viable approach to deliver personalized and informed interactions with users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0568a",
   "metadata": {},
   "source": [
    "## 6. Delete the KDB.AI Table\n",
    "\n",
    "Once finished with the table, it is best practice to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf0e3026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a102d",
   "metadata": {},
   "source": [
    "## Take Our Survey\n",
    "\n",
    "We hope you found this sample helpful! Your feedback is important to us, and we would appreciate it if you could take a moment to fill out our brief survey. Your input helps us improve our content.\n",
    "\n",
    "[**Take the Survey**](https://delighted.com/t/dgCLUkdx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
