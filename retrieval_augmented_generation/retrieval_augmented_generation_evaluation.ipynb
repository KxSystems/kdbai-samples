{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Evaluation with LangChain and KDB.AI\n",
    "\n",
    "##### Note: This example requires a KDB.AI endpoint and API key. Sign up for a free [KDB.AI account](https://kdb.ai/get-started).\n",
    "\n",
    "This notebook serves as a guide to utilizing LangChain tooling for evaluating a basic Retrieval Augmented Generation (RAG) system. \n",
    "\n",
    "The evaluation process involves employing [LangChain's String Evaluators](https://python.langchain.com/docs/guides/evaluation/string/) to assess both conciseness and correctness. KDB.AI serves as the primary knowledge base, enabling the retrieval of semantically relevant content for the evaluation.\n",
    "\n",
    "### Aim\n",
    "\n",
    "In this tutorial, we build upon the retrieval augmented generation pipeline seen in our [retrieval_augmented_generation.ipynb](retrieval_augmented_generation.ipynb) notebook.\n",
    "If you have not seen it, please read and understand that notebook as it will cover the setup steps of RAG in greater detail than we do here.\n",
    "\n",
    "This notebook focuses on the evaluation of your retrieval augmented generation using KDB.AI as the vector store.\n",
    "We will cover the following topics:\n",
    "\n",
    "1. Load Text Data\n",
    "1. Define OpenAI Text Emedding Model\n",
    "1. Store Embeddings In KDB.AI\n",
    "1. Perform Retrieval Augmented Generation\n",
    "1. Evaluate Retrieval Augmented Generation\n",
    "1. Delete the KDB.AI Table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88331c4",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6c97e",
   "metadata": {},
   "source": [
    "### Install dependencies \n",
    "\n",
    "In order to successfully run this sample, note the following steps depending on where you are running this notebook:\n",
    "\n",
    "-***Run Locally / Private Environment:*** The [Setup](https://github.com/KxSystems/kdbai-samples/blob/main/README.md#setup) steps in the repository's `README.md` will guide you on prerequisites and how to run this with Jupyter.\n",
    "\n",
    "\n",
    "-***Colab / Hosted Environment:*** Open this notebook in Colab and run through the cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c93b2276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://kx-user-read:****@ext-nexus.kxi-dev.kx.com/repository/kxi/simple\n",
      "Requirement already satisfied: kdbai_client in /home/gflood/.local/lib/python3.10/site-packages (1.4.0.dev10)\n",
      "Requirement already satisfied: langchain in /home/gflood/.local/lib/python3.10/site-packages (0.1.15)\n",
      "Requirement already satisfied: langchain_openai in /home/gflood/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: langchain-community in /home/gflood/.local/lib/python3.10/site-packages (0.0.32)\n",
      "Requirement already satisfied: packaging in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (23.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (2.1.4)\n",
      "Requirement already satisfied: pykx<3.0.0,>=2.1.1 in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (2.5.1)\n",
      "Requirement already satisfied: requests in /home/gflood/.local/lib/python3.10/site-packages (from kdbai_client) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (3.10.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /home/gflood/.local/lib/python3.10/site-packages (from langchain_openai) (1.37.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /home/gflood/.local/lib/python3.10/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gflood/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gflood/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/gflood/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/gflood/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/gflood/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/gflood/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gflood/.local/lib/python3.10/site-packages (from pandas>=1.5.0->kdbai_client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas>=1.5.0->kdbai_client) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gflood/.local/lib/python3.10/site-packages (from pandas>=1.5.0->kdbai_client) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gflood/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/gflood/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: toml~=0.10.2 in /home/gflood/.local/lib/python3.10/site-packages (from pykx<3.0.0,>=2.1.1->kdbai_client) (0.10.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gflood/.local/lib/python3.10/site-packages (from requests->kdbai_client) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/gflood/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/gflood/.local/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2024.7.24)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gflood/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gflood/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/gflood/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gflood/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->kdbai_client) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gflood/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kdbai_client langchain langchain_openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c95778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\n",
      "--2024-09-25 14:27:09--  https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39027 (38K) [text/plain]\n",
      "Saving to: ‘./data/state_of_the_union.txt.3’\n",
      "\n",
      "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2024-09-25 14:27:09 (4.56 MB/s) - ‘./data/state_of_the_union.txt.3’ saved [39027/39027]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n",
    "### This downloads State of the Union Speech data\n",
    "!mkdir ./data \n",
    "!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679126f7",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "894980f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9549fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ab423cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation packages\n",
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc263a6e",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "To follow this example you will need to request an [OpenAI API Key](https://platform.openai.com/apps). \n",
    "\n",
    "You can create this for free by registering using the links provided.\n",
    "Once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed70fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56faa93",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b03039cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d: dict) -> None:\n",
    "    for k, v in d.items():\n",
    "        print(f\"\\n{k.capitalize()}\\n---\\n{v}\".replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f0b99",
   "metadata": {},
   "source": [
    "## 1. Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aa63a",
   "metadata": {},
   "source": [
    "### Read In Text Document\n",
    "\n",
    "The document we will use for this examples is a State of the Union message from the President of the United States to the United States Congress.\n",
    "\n",
    "In the below code snippet, we read the text file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69dfbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents we want to prompt an LLM about\n",
    "doc = TextLoader(\"data/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed001b92",
   "metadata": {},
   "source": [
    "### Split The Document Into Chunks\n",
    "\n",
    "We then split this document into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84bfd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9c70879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "pages = [p.page_content for p in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cf6a4",
   "metadata": {},
   "source": [
    "## 2. Define OpenAI Text Embedding Model\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database. We select `text-embedding-ada-002` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffa379e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287e75d",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings In KDB.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f5b2f",
   "metadata": {},
   "source": [
    "With the embeddings created, we need to store them in a vector database to enable efficient searching.\n",
    "\n",
    "### Define KDB.AI Session\n",
    "\n",
    "KDB.AI comes in two offerings:\n",
    "\n",
    "1. [KDB.AI Cloud](https://trykdb.kx.com/kdbai/signup/) - For experimenting with smaller generative AI projects with a vector database in our cloud.\n",
    "2. [KDB.AI Server](https://trykdb.kx.com/kdbaiserver/signup/) - For evaluating large scale generative AI applications on-premises or on your own cloud provider.\n",
    "\n",
    "Depending on which you use there will be different setup steps and connection details required.\n",
    "\n",
    "##### Option 1. KDB.AI Cloud\n",
    "\n",
    "To use KDB.AI Cloud, you will need two session details - a URL endpoint and an API key.\n",
    "To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
    "\n",
    "You can connect to a KDB.AI Cloud session using `kdbai.Session` and passing the session URL endpoint and API key details from your KDB.AI Cloud portal.\n",
    "\n",
    "If the environment variables `KDBAI_ENDPOINTS` and `KDBAI_API_KEY` exist on your system containing your KDB.AI Cloud portal details, these variables will automatically be used to connect.\n",
    "If these do not exist, it will prompt you to enter your KDB.AI Cloud portal session URL endpoint and API key details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e62f00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    "    if \"KDBAI_ENDPOINT\" in os.environ\n",
    "    else input(\"KDB.AI endpoint: \")\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    "    if \"KDBAI_API_KEY\" in os.environ\n",
    "    else getpass(\"KDB.AI API key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecf10fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are running a development version of `kdbai_client`.\n",
      "Compatibility with the KDB.AI server is not guaranteed.\n"
     ]
    }
   ],
   "source": [
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712045b1",
   "metadata": {},
   "source": [
    "##### Option 2. KDB.AI Server\n",
    "\n",
    "To use KDB.AI Server, you will need download and run your own container.\n",
    "To do this, you will first need to sign up for free [here](https://trykdb.kx.com/kdbaiserver/signup/). \n",
    "\n",
    "You will receive an email with the required license file and bearer token needed to download your instance.\n",
    "Follow instructions in the signup email to get your session up and running.\n",
    "\n",
    "Once the [setup steps](https://code.kx.com/kdbai/gettingStarted/kdb-ai-server-setup.html) are complete you can then connect to your KDB.AI Server session using `kdbai.Session` and passing your local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a62fd9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = kdbai.Session(endpoint=\"http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d72b5b",
   "metadata": {},
   "source": [
    "### Define Vector DB Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "299902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_schema = [\n",
    "    {\"name\": \"id\", \"type\": \"str\"},\n",
    "    {\"name\": \"text\", \"type\": \"bytes\"},\n",
    "    {\"name\": \"embeddings\", \"type\": \"float32\"}\n",
    "]\n",
    "indexes = [{\"name\": \"flat_index\", \"type\": \"flat\", \"column\": \"embeddings\", \"params\": {\"dims\": 1536, \"metric\": \"L2\"}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fceb2",
   "metadata": {},
   "source": [
    "### Create Vector DB Table\n",
    "\n",
    "Use the KDB.AI `create_table` function to create a table that matches the defined schema in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bbc9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = session.database(\"default\")\n",
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    database.table(\"rag_eval\").drop()\n",
    "except kdbai.KDBAIException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37840395",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = database.create_table(\"rag_eval\", schema=rag_eval_schema, indexes=indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934da954",
   "metadata": {},
   "source": [
    "### Add Embedded Data to KDB.AI Table\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.from_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7680e758",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Table.insert() got an unexpected keyword argument 'warn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use KDBAI as vector store\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vecdb_kdbai \u001b[38;5;241m=\u001b[39m KDBAI(table, embeddings)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvecdb_kdbai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/kdbai.py:150\u001b[0m, in \u001b[0;36mKDBAI.add_texts\u001b[0;34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         batch_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     out_ids \u001b[38;5;241m=\u001b[39m out_ids \u001b[38;5;241m+\u001b[39m batch_ids\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_ids\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/kdbai.py:97\u001b[0m, in \u001b[0;36mKDBAI._insert\u001b[0;34m(self, texts, ids, metadata)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, metadata], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Table.insert() got an unexpected keyword argument 'warn'"
     ]
    }
   ],
   "source": [
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "vecdb_kdbai.add_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8d806",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569892",
   "metadata": {},
   "source": [
    "## 4. Perform Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a0636",
   "metadata": {},
   "source": [
    "We will perform [question answering (QA) in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4) using `RetrievalQA`.\n",
    "\n",
    "`RetrievalQA` retrieves the most relevant chunk of text and does QA on that subset.\n",
    "We will use KDB.AI as the retriever of `RetrievalQA`.\n",
    "\n",
    "### Define QA Bot\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-4o-mini for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9011f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ca7342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qabot = RetrievalQA.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ee6a4",
   "metadata": {},
   "source": [
    "`as_retriever` is a method that converts a vectorstore into a retriever. A retriever is an interface that returns documents given an unstructured query. By using <code>as_retriever</code>, we can create a retriever from a vectorstore and use it to retrieve relevant documents for a query. This allows us to perform question answering over the documents indexed by the vectorstore `vecdb_kdbai`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057670d5",
   "metadata": {},
   "source": [
    "### Query The QA Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de98d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qabot(qabot, query: str) -> str:\n",
    "    query_res = qabot.invoke(dict(query=query))[\"result\"]\n",
    "    print(f\"{query}\\n---\\n{query_res}\")\n",
    "    return query_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ca2ca",
   "metadata": {},
   "source": [
    "##### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ca7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What improvements could be made in infrastructure?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd88bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What improvements could be made in infrastructure?\n",
      "---\n",
      "Some improvements that could be made in infrastructure include:\n",
      "\n",
      "1. Rebuilding and repairing roads, bridges, and highways that are in disrepair.\n",
      "2. Building a national network of 500,000 electric vehicle charging stations.\n",
      "3. Replacing poisonous lead pipes to ensure clean water for every American.\n",
      "4. Providing affordable high-speed internet access for all Americans, including urban, suburban, rural, and tribal communities.\n",
      "5. Modernizing airports, ports, and waterways.\n",
      "6. Investing in renewable energy production, such as solar and wind, to promote clean energy and reduce reliance on fossil fuels.\n",
      "7. Weatherizing homes and businesses to improve energy efficiency and reduce costs.\n",
      "8. Investing in emerging technologies and American manufacturing to compete with global competitors like China.\n",
      "9. Ensuring that infrastructure projects are made in America, supporting domestic manufacturing and supply chains.\n",
      "10. Increasing investments in crime prevention and community police officers to improve safety and restore trust in law enforcement.\n",
      "\n",
      "These are just a few examples, and there may be other specific improvements that can be made depending on the needs of different regions and communities.\n"
     ]
    }
   ],
   "source": [
    "res1 = query_qabot(qabot, query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f329b3",
   "metadata": {},
   "source": [
    "##### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0eece5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"How many jobs were created in the country due the electric vehicle manufacturing industry?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41997198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many jobs were created in the country due the electric vehicle manufacturing industry?\n",
      "---\n",
      "The passage states that Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. Additionally, GM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. Therefore, a total of 15,000 jobs were created in the country due to the electric vehicle manufacturing industry mentioned in the passage.\n"
     ]
    }
   ],
   "source": [
    "res2 = query_qabot(qabot, query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d38581",
   "metadata": {},
   "source": [
    "## 5. Evaluate Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933ae8c",
   "metadata": {},
   "source": [
    "Here we will carry out two evaluation techniques against the results of our retrieval augmented generation pipeline.\n",
    "We will measure the *Conciseness* and the *Correctness* of the answers.\n",
    "\n",
    "### Evaluate Conciseness\n",
    "\n",
    "We will evaluate the conciseness of the answers the QA bot returns using LangChain's `load_evaluator` function with the `criteria` set to `\"conciseness\"`.\n",
    "\n",
    "In this example, we use GPT-4o as the LLM that performs the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c14a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41ed25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_evaluator = load_evaluator(\n",
    "    \"criteria\", criteria=\"conciseness\", llm=evaluation_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a63b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_eval_res = concise_evaluator.evaluate_strings(prediction=res1, input=query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7866960-9256-4df2-8087-49dcf43c3124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning\n",
      "---\n",
      "The criterion for assessment is the conciseness of the submitted answer. \n",
      "The submission gives a list of ten potential improvements to infrastructure. Each suggestion is fairly concise, providing a brief explanation of the proposed improvement without unnecessary elaboration or tangents. \n",
      "However, the submission does include an introductory sentence and a concluding sentence that add some length. The conclusion, in particular, adds a bit of extra information about the potential for other improvements depending on regional needs.\n",
      "This additional information could be seen as unnecessary, but it also provides context and acknowledges the complexity of infrastructure improvements, which could be seen as enhancing the quality of the response rather than detracting from its conciseness.\n",
      "Overall, while not the briefest possible response, the submission is fairly concise and to the point. Each suggested improvement is described in a single, succinct sentence, and the overall response does not stray from the topic of infrastructure improvements.\n",
      "Y\n",
      "\n",
      "Value\n",
      "---\n",
      "Y\n",
      "\n",
      "Score\n",
      "---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print_dict(concise_eval_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e4e74-876c-4dae-9e0d-42f20698ea43",
   "metadata": {},
   "source": [
    "### Evaluate Correctness\n",
    "\n",
    "We can use the same `load_evaluator` function to calculate correctness by simply changing the `criteria` to `\"correctness\"`.\n",
    "\n",
    "When using this option, we can pass a reference for the evaluator to check the correctness against.\n",
    "Let's pass a reference that matches the information returned as well as one that doesn't.\n",
    "\n",
    "For this evaluation, we will use the result of the second query we ran through our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d5742f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_evaluator = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"correctness\",\n",
    "    llm=evaluation_llm,\n",
    "    requires_reference=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fe16e",
   "metadata": {},
   "source": [
    "##### Matching Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86e41652",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ref = \"15000 jobs were created due to manufacturing of electric vehicles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2bd3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_eval_res1 = correct_evaluator.evaluate_strings(\n",
    "    prediction=res2, input=query2, reference=matching_ref\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "708d3386-f28d-4a6a-bb7c-10a15e8574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning\n",
      "---\n",
      "First, we need to assess the correctness of the submission, according to the criteria.\n",
      "The input asks about the number of jobs created in the country due to the electric vehicle manufacturing industry.\n",
      "The submission provides a detailed answer, stating that Ford and GM's investments in electric vehicles have created a total of 15,000 jobs across the country.\n",
      "Comparing this to the reference, which states that 15,000 jobs were created due to the manufacturing of electric vehicles, it's clear that the submission is accurate and factual.\n",
      "Therefore, the submission meets the criteria.\n",
      "Y\n",
      "\n",
      "Value\n",
      "---\n",
      "Y\n",
      "\n",
      "Score\n",
      "---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print_dict(correct_eval_res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a317f",
   "metadata": {},
   "source": [
    "##### Contradictory Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2b8c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractic_ref = \"12000 jobs were created due to manufacturing of electric vehicles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea0cb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_eval_res2 = correct_evaluator.evaluate_strings(\n",
    "    prediction=res2, input=query2, reference=contractic_ref\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2172b83f-61ca-4963-8225-0378580a67a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning\n",
      "---\n",
      "The criteria for this task is correctness: Is the submission correct, accurate, and factual?\n",
      "Looking at the submission, the answer provided is that 15,000 jobs were created due to the electric vehicle manufacturing industry. This is based on the data provided in the submission that Ford created 11,000 jobs and GM created 4,000 jobs.\n",
      "The reference data, however, states that 12,000 jobs were created due to the manufacturing of electric vehicles.\n",
      "Since the submission and the reference data do not match, it appears that the submission does not meet the criteria of correctness. The submission's answer is not accurate according to the reference data.\n",
      "N\n",
      "\n",
      "Value\n",
      "---\n",
      "N\n",
      "\n",
      "Score\n",
      "---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_dict(correct_eval_res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195efbb",
   "metadata": {},
   "source": [
    "## 6. Delete the KDB.AI Table\n",
    "\n",
    "Once finished with the table, it is best practice to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d83ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed75e9",
   "metadata": {},
   "source": [
    "## Take Our Survey\n",
    "\n",
    "We hope you found this sample helpful! Your feedback is important to us, and we would appreciate it if you could take a moment to fill out our brief survey. Your input helps us improve our content.\n",
    "\n",
    "[**Take the Survey**](https://delighted.com/t/dgCLUkdx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
