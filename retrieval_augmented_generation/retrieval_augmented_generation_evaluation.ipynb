{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Evaluation with LangChain and KDB.AI\n",
    "\n",
    "This notebook serves as a guide to utilizing LangChain tooling for evaluating a basic Retrieval Augmented Generation (RAG) system. \n",
    "\n",
    "The evaluation process involves employing [LangChain's String Evaluators](https://python.langchain.com/docs/guides/evaluation/string/) to assess both conciseness and correctness. KDB.AI serves as the primary knowledge base, enabling the retrieval of semantically relevant content for the evaluation.\n",
    "\n",
    "### Aim\n",
    "\n",
    "In this tutorial, we build upon the retrieval augmented generation pipeline seen in our [retrieval_augmented_generation.ipynb](retrieval_augmented_generation.ipynb) notebook.\n",
    "If you have not seen it, please read and understand that notebook as it will cover the setup steps of RAG in greater detail than we do here.\n",
    "\n",
    "This notebook focuses on the evaluation of your retrieval augmented generation using KDB.AI as the vector store.\n",
    "We will cover the following topics:\n",
    "\n",
    "1. Load Text Data\n",
    "1. Define OpenAI Text Emedding Model\n",
    "1. Store Embeddings In KDB.AI\n",
    "1. Perform Retrieval Augmented Generation\n",
    "1. Evaluate Retrieval Augmented Generation\n",
    "1. Delete the KDB.AI Table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88331c4",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6c97e",
   "metadata": {},
   "source": [
    "### Install dependencies \n",
    "\n",
    "In order to successfully run this sample, the [Setup](https://github.com/KxSystems/kdbai-samples/blob/main/README.md#setup) steps in the repository's `README.md` file must be completed.\n",
    "This will ensure that you have installed all of the relevant packages and versions needed for this sample.\n",
    "If you have not completed these setup steps, please navigate to the repositories `README.md` file and follow the steps detailed there.\n",
    "\n",
    "### Import Packages\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kdbai_client langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95778f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! Only run this cell if running the notebook in Colab\n",
    "### This downloads state of the union speech into Colab\n",
    "!mkdir ./data \n",
    "!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894980f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9549fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab423cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation packages\n",
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc263a6e",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    "\n",
    "To follow this example you will need to request an [OpenAI API Key](https://platform.openai.com/apps). \n",
    "\n",
    "You can create this for free by registering using the links provided.\n",
    "Once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed70fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56faa93",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03039cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d: dict) -> None:\n",
    "    for k, v in d.items():\n",
    "        print(f\"\\n{k.capitalize()}\\n---\\n{v}\".replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f0b99",
   "metadata": {},
   "source": [
    "## 1. Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aa63a",
   "metadata": {},
   "source": [
    "### Read In Text Document\n",
    "\n",
    "The document we will use for this examples is a State of the Union message from the President of the United States to the United States Congress.\n",
    "\n",
    "In the below code snippet, we read the text file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dfbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents we want to prompt an LLM about\n",
    "doc = TextLoader(\"data/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed001b92",
   "metadata": {},
   "source": [
    "### Split The Document Into Chunks\n",
    "\n",
    "We then split this document into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84bfd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c70879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "pages = [p.page_content for p in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cf6a4",
   "metadata": {},
   "source": [
    "## 2. Define OpenAI Text Embedding Model\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database. We select `text-embedding-ada-002` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa379e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287e75d",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings In KDB.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f5b2f",
   "metadata": {},
   "source": [
    "With the embeddings created, we need to store them in a vector database to enable efficient searching.\n",
    "\n",
    "### Define KDB.AI Session\n",
    "\n",
    "KDB.AI comes in two offerings:\n",
    "\n",
    "1. [KDB.AI Cloud](https://trykdb.kx.com/kdbai/signup/) - For experimenting with smaller generative AI projects with a vector database in our cloud.\n",
    "2. [KDB.AI Server](https://trykdb.kx.com/kdbaiserver/signup/) - For evaluating large scale generative AI applications on-premises or on your own cloud provider.\n",
    "\n",
    "Depending on which you use there will be different setup steps and connection details required.\n",
    "\n",
    "##### Option 1. KDB.AI Cloud\n",
    "\n",
    "To use KDB.AI Cloud, you will need two session details - a URL endpoint and an API key.\n",
    "To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
    "\n",
    "You can connect to a KDB.AI Cloud session using `kdbai.Session` and passing the session URL endpoint and API key details from your KDB.AI Cloud portal.\n",
    "\n",
    "If the environment variables `KDBAI_ENDPOINTS` and `KDBAI_API_KEY` exist on your system containing your KDB.AI Cloud portal details, these variables will automatically be used to connect.\n",
    "If these do not exist, it will prompt you to enter your KDB.AI Cloud portal session URL endpoint and API key details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62f00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    "    if \"KDBAI_ENDPOINT\" in os.environ\n",
    "    else input(\"KDB.AI endpoint: \")\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    "    if \"KDBAI_API_KEY\" in os.environ\n",
    "    else getpass(\"KDB.AI API key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf10fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712045b1",
   "metadata": {},
   "source": [
    "##### Option 2. KDB.AI Server\n",
    "\n",
    "To use KDB.AI Server, you will need download and run your own container.\n",
    "To do this, you will first need to sign up for free [here](https://trykdb.kx.com/kdbaiserver/signup/). \n",
    "\n",
    "You will receive an email with the required license file and bearer token needed to download your instance.\n",
    "Follow instructions in the signup email to get your session up and running.\n",
    "\n",
    "Once the [setup steps](https://code.kx.com/kdbai/gettingStarted/kdb-ai-server-setup.html) are complete you can then connect to your KDB.AI Server session using `kdbai.Session` and passing your local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62fd9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = kdbai.Session(endpoint=\"http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d72b5b",
   "metadata": {},
   "source": [
    "### Define Vector DB Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"id\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"text\", \"pytype\": \"bytes\"},\n",
    "        {\n",
    "            \"name\": \"embeddings\",\n",
    "            \"pytype\": \"float32\",\n",
    "            \"vectorIndex\": {\"dims\": 1536, \"metric\": \"L2\", \"type\": \"flat\"},\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fceb2",
   "metadata": {},
   "source": [
    "### Create Vector DB Table\n",
    "\n",
    "Use the KDB.AI `create_table` function to create a table that matches the defined schema in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbc9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"rag_eval\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37840395",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = session.create_table(\"rag_eval\", rag_eval_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934da954",
   "metadata": {},
   "source": [
    "### Add Embedded Data to KDB.AI Table\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.from_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7680e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "vecdb_kdbai.add_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8d806",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569892",
   "metadata": {},
   "source": [
    "## 4. Perform Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a0636",
   "metadata": {},
   "source": [
    "We will perform [question answering (QA) in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4) using `RetrievalQA`.\n",
    "\n",
    "`RetrievalQA` retrieves the most relevant chunk of text and does QA on that subset.\n",
    "We will use KDB.AI as the retriever of `RetrievalQA`.\n",
    "\n",
    "### Define QA Bot\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-3.5 Turbo for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9011f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ca7342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qabot = RetrievalQA.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.0),\n",
    "    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ee6a4",
   "metadata": {},
   "source": [
    "`as_retriever` is a method that converts a vectorstore into a retriever. A retriever is an interface that returns documents given an unstructured query. By using <code>as_retriever</code>, we can create a retriever from a vectorstore and use it to retrieve relevant documents for a query. This allows us to perform question answering over the documents indexed by the vectorstore `vecdb_kdbai`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057670d5",
   "metadata": {},
   "source": [
    "### Query The QA Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de98d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qabot(qabot, query: str) -> str:\n",
    "    query_res = qabot.invoke(dict(query=query))[\"result\"]\n",
    "    print(f\"{query}\\n---\\n{query_res}\")\n",
    "    return query_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ca2ca",
   "metadata": {},
   "source": [
    "##### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ca7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What improvements could be made in infrastructure?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd88bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What improvements could be made in infrastructure?\n",
      "---\n",
      "Some improvements that could be made in infrastructure include:\n",
      "\n",
      "1. Rebuilding and repairing roads, bridges, and highways that are in disrepair.\n",
      "2. Building a national network of 500,000 electric vehicle charging stations.\n",
      "3. Replacing poisonous lead pipes to ensure clean water for every American.\n",
      "4. Providing affordable high-speed internet access for all Americans, including urban, suburban, rural, and tribal communities.\n",
      "5. Modernizing airports, ports, and waterways.\n",
      "6. Investing in renewable energy production, such as solar and wind, to promote clean energy and reduce reliance on fossil fuels.\n",
      "7. Weatherizing homes and businesses to improve energy efficiency and reduce costs.\n",
      "8. Investing in emerging technologies and American manufacturing to compete with global competitors like China.\n",
      "9. Ensuring that infrastructure projects are made in America, supporting domestic manufacturing and supply chains.\n",
      "10. Increasing investments in crime prevention and community police officers to improve safety and restore trust in law enforcement.\n",
      "\n",
      "These are just a few examples, and there may be other specific improvements that can be made depending on the needs of different regions and communities.\n"
     ]
    }
   ],
   "source": [
    "res1 = query_qabot(qabot, query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f329b3",
   "metadata": {},
   "source": [
    "##### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0eece5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"How many jobs were created in the country due the electric vehicle manufacturing industry?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41997198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many jobs were created in the country due the electric vehicle manufacturing industry?\n",
      "---\n",
      "The passage states that Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. Additionally, GM is making the largest investment in its historyâ€”$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. Therefore, a total of 15,000 jobs were created in the country due to the electric vehicle manufacturing industry mentioned in the passage.\n"
     ]
    }
   ],
   "source": [
    "res2 = query_qabot(qabot, query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d38581",
   "metadata": {},
   "source": [
    "## 5. Evaluate Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933ae8c",
   "metadata": {},
   "source": [
    "Here we will carry out two evaluation techniques against the results of our retrieval augmented generation pipeline.\n",
    "We will measure the *Conciseness* and the *Correctness* of the answers.\n",
    "\n",
    "### Evaluate Conciseness\n",
    "\n",
    "We will evaluate the conciseness of the answers the QA bot returns using LangChain's `load_evaluator` function with the `criteria` set to `\"conciseness\"`.\n",
    "\n",
    "In this example, we use GPT-4 as the LLM that performs the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c14a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41ed25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_evaluator = load_evaluator(\n",
    "    \"criteria\", criteria=\"conciseness\", llm=evaluation_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a63b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_eval_res = concise_evaluator.evaluate_strings(prediction=res1, input=query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7866960-9256-4df2-8087-49dcf43c3124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning\n",
      "---\n",
      "The criterion for assessment is the conciseness of the submitted answer. \n",
      "The submission gives a list of ten potential improvements to infrastructure. Each suggestion is fairly concise, providing a brief explanation of the proposed improvement without unnecessary elaboration or tangents. \n",
      "However, the submission does include an introductory sentence and a concluding sentence that add some length. The conclusion, in particular, adds a bit of extra information about the potential for other improvements depending on regional needs.\n",
      "This additional information could be seen as unnecessary, but it also provides context and acknowledges the complexity of infrastructure improvements, which could be seen as enhancing the quality of the response rather than detracting from its conciseness.\n",
      "Overall, while not the briefest possible response, the submission is fairly concise and to the point. Each suggested improvement is described in a single, succinct sentence, and the overall response does not stray from the topic of infrastructure improvements.\n",
      "Y\n",
      "\n",
      "Value\n",
      "---\n",
      "Y\n",
      "\n",
      "Score\n",
      "---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print_dict(concise_eval_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e4e74-876c-4dae-9e0d-42f20698ea43",
   "metadata": {},
   "source": [
    "### Evaluate Correctness\n",
    "\n",
    "We can use the same `load_evaluator` function to calculate correctness by simply changing the `criteria` to `\"correctness\"`.\n",
    "\n",
    "When using this option, we can pass a reference for the evaluator to check the correctness against.\n",
    "Let's pass a reference that matches the information returned as well as one that doesn't.\n",
    "\n",
    "For this evaluation, we will use the result of the second query we ran through our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d5742f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_evaluator = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"correctness\",\n",
    "    llm=evaluation_llm,\n",
    "    requires_reference=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fe16e",
   "metadata": {},
   "source": [
    "##### Matching Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86e41652",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ref = \"15000 jobs were created due to manufacturing of electric vehicles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2bd3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_eval_res1 = correct_evaluator.evaluate_strings(\n",
    "    prediction=res2, input=query2, reference=matching_ref\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "708d3386-f28d-4a6a-bb7c-10a15e8574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning\n",
      "---\n",
      "First, we need to assess the correctness of the submission, according to the criteria.\n",
      "The input asks about the number of jobs created in the country due to the electric vehicle manufacturing industry.\n",
      "The submission provides a detailed answer, stating that Ford and GM's investments in electric vehicles have created a total of 15,000 jobs across the country.\n",
      "Comparing this to the reference, which states that 15,000 jobs were created due to the manufacturing of electric vehicles, it's clear that the submission is accurate and factual.\n",
      "Therefore, the submission meets the criteria.\n",
      "Y\n",
      "\n",
      "Value\n",
      "---\n",
      "Y\n",
      "\n",
      "Score\n",
      "---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print_dict(correct_eval_res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a317f",
   "metadata": {},
   "source": [
    "##### Contradictory Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2b8c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractic_ref = \"12000 jobs were created due to manufacturing of electric vehicles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea0cb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_eval_res2 = correct_evaluator.evaluate_strings(\n",
    "    prediction=res2, input=query2, reference=contractic_ref\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2172b83f-61ca-4963-8225-0378580a67a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning\n",
      "---\n",
      "The criteria for this task is correctness: Is the submission correct, accurate, and factual?\n",
      "Looking at the submission, the answer provided is that 15,000 jobs were created due to the electric vehicle manufacturing industry. This is based on the data provided in the submission that Ford created 11,000 jobs and GM created 4,000 jobs.\n",
      "The reference data, however, states that 12,000 jobs were created due to the manufacturing of electric vehicles.\n",
      "Since the submission and the reference data do not match, it appears that the submission does not meet the criteria of correctness. The submission's answer is not accurate according to the reference data.\n",
      "N\n",
      "\n",
      "Value\n",
      "---\n",
      "N\n",
      "\n",
      "Score\n",
      "---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_dict(correct_eval_res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195efbb",
   "metadata": {},
   "source": [
    "## 6. Delete the KDB.AI Table\n",
    "\n",
    "Once finished with the table, it is best practice to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d83ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed75e9",
   "metadata": {},
   "source": [
    "## Take Our Survey\n",
    "\n",
    "We hope you found this sample helpful! Your feedback is important to us, and we would appreciate it if you could take a moment to fill out our brief survey. Your input helps us improve our content.\n",
    "\n",
    "[**Take the Survey**](https://delighted.com/t/dgCLUkdx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
