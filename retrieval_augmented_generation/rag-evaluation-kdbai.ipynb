{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# RAG Evaluation Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a2d3c-46f5-40b7-8603-a88fca8fdd2e",
   "metadata": {},
   "source": [
    "## LangChain Conciseness, Correctness\n",
    "\n",
    "Measures how to the point and correct the response is.\n",
    "\n",
    "### 1. Build basic RAG pipeline using GPT-3.5\n",
    "\n",
    "We load a PDF document that contains a state of the union address. We then build a generic QA system using LangChain with KDB.AI as the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa659883-dfc4-499c-adee-6e119815b606",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/KxSystems/langchain.git@KDB.AI#subdirectory=libs/langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a80807-55f5-48d9-bb08-17b1b5887539",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644c0326-933a-499b-9a85-f4d1139c5307",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8ceb98-fab4-48ef-abc9-b11c9a346b51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    "    if \"KDBAI_ENDPOINT\" in os.environ\n",
    "    else input(\"KDB.AI endpoint: \")\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    "    if \"KDBAI_API_KEY\" in os.environ\n",
    "    else getpass(\"KDB.AI API key: \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64393c0-799e-49b1-953f-e545cbcf1b59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import KDBAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "doc = TextLoader(\"data/state_of_the_union.txt\").load()\n",
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "# split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "pages = [p.page_content for p in text_splitter.split_documents(doc)]\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n",
    "\n",
    "rag_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"id\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"text\", \"pytype\": \"bytes\"},\n",
    "        {\n",
    "            \"name\": \"embeddings\",\n",
    "            \"pytype\": \"float32\",\n",
    "            \"vectorIndex\": {\"dims\": 1536, \"metric\": \"L2\", \"type\": \"flat\"},\n",
    "        },\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c13997-9c4d-40d3-9c87-b628332f743f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"rag_langchain\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass\n",
    "\n",
    "table = session.create_table(\"rag_langchain\", rag_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32c984-3c5e-433a-bd6c-3579d8558db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "vecdb_kdbai.add_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dca709a-d403-4fcf-90fb-7716ec820fb9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Because we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history.', metadata={'id': '42886e69-0589-49b5-8de8-3cdac385e545', 'embeddings': array([ 0.0033795 ,  0.01255523,  0.00851544, ..., -0.0206088 ,\n",
       "        -0.0326306 , -0.01865721], dtype=float32)})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = \"What improvements could be made in infrastructure?\"\n",
    "# query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "query_sim = vecdb_kdbai.similarity_search(query2)\n",
    "query_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9ecda2-36e5-4127-80c0-13d208f8f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-3.5 as retriever\n",
    "K = 10\n",
    "qabot = RetrievalQA.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.0),\n",
    "    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aea0742-0c14-4604-87d9-cad99b2cf3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What improvements could be made in infrastructure?\n",
      "-----\n",
      "Some improvements that could be made in infrastructure include:\n",
      "\n",
      "1. Rebuilding and repairing roads, bridges, and highways that are in disrepair.\n",
      "2. Building a national network of 500,000 electric vehicle charging stations.\n",
      "3. Replacing poisonous lead pipes to ensure clean water for every American.\n",
      "4. Providing affordable high-speed internet access for all Americans, including urban, suburban, rural, and tribal communities.\n",
      "5. Modernizing airports, ports, and waterways.\n",
      "6. Investing in renewable energy production, such as solar and wind, to promote clean energy and reduce reliance on fossil fuels.\n",
      "7. Weatherizing homes and businesses to improve energy efficiency and reduce costs.\n",
      "8. Investing in emerging technologies and American manufacturing to compete with global competitors like China.\n",
      "9. Ensuring that infrastructure projects are made in America, supporting domestic manufacturing and supply chains.\n",
      "10. Increasing investments in crime prevention and community police officers to improve safety and restore trust in law enforcement.\n",
      "\n",
      "These are just a few examples, and there may be other specific improvements that can be made depending on the needs of different regions and communities.\n"
     ]
    }
   ],
   "source": [
    "# testing it out \n",
    "print(query2)\n",
    "print(\"-----\")\n",
    "pred = qabot(dict(query=query2))[\"result\"]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964fa1f-c8d4-40cd-86e8-c4f4fb671e14",
   "metadata": {},
   "source": [
    "### 2. Calculate Conciseness using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7866960-9256-4df2-8087-49dcf43c3124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion in question is conciseness. This refers to the '\n",
      "              'submission being brief and directly expressing the points '\n",
      "              'without unnecessary detail. \\n'\n",
      "              '\\n'\n",
      "              'Looking at the submission, it provides a detailed list of 10 '\n",
      "              'possible improvements to infrastructure. Each point is stated '\n",
      "              'directly and briefly, without any excessive elaboration or '\n",
      "              'unnecessary detail. It is also arranged in a clear and '\n",
      "              'organized fashion, which aids in its conciseness. \\n'\n",
      "              '\\n'\n",
      "              'Therefore, the submission can be considered concise and to the '\n",
      "              'point. \\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "source": [
    "#gpt-4 as evaluator\n",
    "from pprint import pprint as print\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluation_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=query2,\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e4e74-876c-4dae-9e0d-42f20698ea43",
   "metadata": {},
   "source": [
    "### 3. Calculate Correctness using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6776b8ba-208f-47d3-ab7c-bc2e897fd48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How many jobs were created in the country due the electric vehicle '\n",
      " 'manufacturing industry?')\n",
      "'-----'\n",
      "('The passage states that Ford is investing $11 billion to build electric '\n",
      " 'vehicles, creating 11,000 jobs across the country. Additionally, GM is '\n",
      " 'making the largest investment in its history—$7 billion to build electric '\n",
      " 'vehicles, creating 4,000 jobs in Michigan. Therefore, a total of 15,000 jobs '\n",
      " 'were created in the country due to the electric vehicle manufacturing '\n",
      " 'industry mentioned in the passage.')\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How many jobs were created in the country due the electric vehicle manufacturing industry?\"\n",
    "print(query3)\n",
    "print(\"-----\")\n",
    "pred3 = qabot(dict(query=query3))[\"result\"]\n",
    "print(pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "708d3386-f28d-4a6a-bb7c-10a15e8574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion for this task is the correctness, accuracy, and '\n",
      "              'factualness of the submitted answer. \\n'\n",
      "              '\\n'\n",
      "              'The reference states that 15000 jobs were created due to the '\n",
      "              'manufacturing of electric vehicles. The submission states the '\n",
      "              'same result, mentioning that Ford created 11,000 jobs and GM '\n",
      "              'created 4,000 jobs, therefore a total of 15,000 jobs were '\n",
      "              'created. \\n'\n",
      "              '\\n'\n",
      "              'The submission matches the reference and appears to be accurate '\n",
      "              'and factual, suggesting that the submission meets the '\n",
      "              'criterion. \\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "source": [
    "# reference matches\n",
    "evaluator3 = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm,requires_reference=True)\n",
    "\n",
    "eval_result3 = evaluator3.evaluate_strings(\n",
    "    prediction=pred3,\n",
    "    input=query3,\n",
    "    reference=\"15000 jobs were created due to manufacturing of electric vehicles.\"\n",
    ")\n",
    "\n",
    "print(eval_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2172b83f-61ca-4963-8225-0378580a67a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion for this assessment is the correctness of the '\n",
      "              'submission. We have to determine if the submission is accurate '\n",
      "              'and factual. The submission states that due to the electric '\n",
      "              'vehicle manufacturing industry, 15,000 jobs were created. '\n",
      "              'However, the reference provided states that there were 12,000 '\n",
      "              'jobs created due to the manufacturing of electric vehicles. '\n",
      "              'Therefore, the submission does not match the reference in terms '\n",
      "              'of the number of jobs created. Hence, the submission does not '\n",
      "              'meet the correctness criterion.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "# reference contradicts\n",
    "eval_result4 = evaluator3.evaluate_strings(\n",
    "    prediction=pred3,\n",
    "    input=query3,\n",
    "    reference=\"12000 jobs were created due to manufacturing of electric vehicles.\"\n",
    ")\n",
    "\n",
    "print(eval_result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc14a30-c15f-41f9-baf6-65e578be1ac0",
   "metadata": {},
   "source": [
    "## Other Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba576b3a-786c-4999-b7ef-c344b7c75553",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c6ff6-a2fd-4a7d-b98e-03111d01b12a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12721827-1fd0-4cb8-ba76-f1e9d81ee3fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!python3 -c \"import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca32384-5f47-414c-ab40-fd48abb8062e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e209cb63-61d0-4cdb-b3fd-fe5bb03e0ab1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c66a22c9-66b2-4ce9-9e87-3ff86a1f24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = [\"The new study found that eating chocolate can improve cognitive function.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64cd3801-857d-43b1-b119-d90f63b70443",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary1 = [\"Chocolate can improve brain health.\"]\n",
    "summary2 = [\"A new study suggests that chocolate consumption can boost cognitive performance.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1773d3a-96d0-4d53-ab22-344b65a9f741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.25, 0.09090909090909091, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.0,\n",
       " 'translation_length': 12,\n",
       " 'reference_length': 6}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=original, references=summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf4aec53-5b95-4b81-84d3-6818c07670a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.5833333333333334, 0.09090909090909091, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0,\n",
       " 'translation_length': 12,\n",
       " 'reference_length': 12}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=original, references=summary2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b77ed-d024-4ab1-b2b8-33b0821a9325",
   "metadata": {},
   "source": [
    "BLEU favors Summary 2 likely because it has a higher proportion of words that match the original article.​​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ccaf1-75e8-496a-8c97-7e1d589315fe",
   "metadata": {},
   "source": [
    "BLEU’s output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts. Few human translations will attain a score of 1, since this would indicate that the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a score of 1. \n",
    "\n",
    "Let's test this by giving it the identical text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c447f73-0dc3-478c-af13-82a4208d24c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 1.0,\n",
       " 'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0,\n",
       " 'translation_length': 12,\n",
       " 'reference_length': 12}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=original, references=[\"The new study found that eating chocolate can improve cognitive function.\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
